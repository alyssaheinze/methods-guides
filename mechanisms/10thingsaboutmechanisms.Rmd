---
output:
  bookdown::html_document2:
    number_sections: true
    toc: true
    theme: journal
    extensions: +implicit_figure
    code_folding: show
    fig_caption: true
    pandoc_args: [
           --filter, pandoc-crossref
        ]
---

<!-- title: "10 choses sur les mécanismes" -->
<!-- author: 'Auteur du guide des méthodes: Lindsay Dolan' -->

```{r echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
require(knitr)
opts_chunk$set(strip.white=TRUE,
               width.cutoff=132,
               size='\\scriptsize',
               out.width='.9\\textwidth',
               message=FALSE,
               warning=FALSE,
               echo=TRUE,
               comment=NA,
               tidy='styler',
               prompt=FALSE,
               results='markup')

```

En tant que spécialistes en sciences sociales, nous sommes fascinés par les questions causales.
Dès que nous apprenons que X cause Y, nous voulons mieux comprendre *pourquoi* X cause Y.
Ce guide explore le rôle des « mécanismes » dans l'analyse causale et vous aidera à comprendre les types de conclusions que vous pouvez en tirer.

# Les mécanismes sont des voies par lesquelles X provoque le résultat Y.

Les mécanismes sont depuis longtemps au cœur de la médecine.
Chaque fois qu'un médecin prescrit un traitement, elle le fait en sachant quels facteurs chimiques ou physiques provoquent une maladie, et elle prescrit un traitement efficace car il interrompt ces facteurs.
Par exemple, de nombreux psychologues cliniciens recommandent l'exercice aux patients souffrant de dépression.
L'exercice augmente les endorphines dans la chimie du corps, qui déclenchent des sentiments positifs et agissent également comme analgésiques, ce qui réduit la perception de la douleur.
Les endorphines sont donc un mécanisme par lequel l'exercice aide à réduire la dépression.
L'exercice peut avoir des effets positifs sur un certain nombre d'autres variables dépendantes (par exemple, les maladies cardiaques) par le biais d'autres mécanismes (par exemple, l'élévation du rythme cardiaque), mais le mécanisme qui l'amène à affecter la dépression en particulier est l'endorphine.
Nous pourrions également conclure qu'un autre traitement, tel qu'un médicament qui augmente les endorphines, peut avoir des effets similaires sur la dépression.

Les mécanismes sont tout aussi importants pour les sciences sociales.
Prenez, par exemple, des recherches récentes qui ont lié le changement climatique à une augmentation des conflits civils.
Une étude[^1] prétend identifier l'effet causal des chocs climatiques sur les conflits violents en étudiant le taux de conflits civils dans les pays touchés par El Niño pendant les années El Niño par rapport aux années sans El Niño.
Supposons que cette étude soit correcte.
Pourquoi le fait de subir un choc climatique entraînerait-il dans un pays des niveaux de conflit élevés ? Un mécanisme pourrait être la pauvreté : les chocs climatiques nuisent à l'économie, et avec des coûts d'opportunité inférieurs, les individus sont plus enclins à rejoindre des groupes armés.
Un mécanisme alternatif est physiologique : les gens sont physiquement câblés pour être plus agressifs par temps chaud.
Le mécanisme est peut-être la migration : les chocs climatiques déplacent les populations des régions côtières, ce qui produit des conflits sociaux entre migrants et autochtones.
En réalité, plusieurs ou tous ces mécanismes (ainsi que d'autres non listés ici) pourraient fonctionner simultanément, même dans le même cas ! Dans bon nombre des questions les plus intéressantes en sciences sociales, il existe plusieurs canaux (« M ») qui pourraient transmettre l'effet total de X sur Y.

[^1]: Solomon M. Hsiang, Kyle C. Meng, and Mark A. Cane, “Civil Conflicts Are Associated with the Global Climate,” Nature 476.7361 (2011): 438-441.

# Bien que nous n'ayons pas *besoin* de connaître le mécanisme pour conclure que X cause Y, il y a plusieurs raisons pour lesquelles nous *voudrions* le faire.

Dans l'exemple climat/conflit ci-dessus, nous pouvons avoir une confiance totale dans la capacité des chercheurs à identifier causalement que les chocs climatiques provoquent des conflits, et pourtant n'avons aucune preuve du ou des mécanismes à l'œuvre.
Mais les chercheurs en sciences sociales sont intéressés à en apprendre davantage sur les mécanismes parce qu'ils sont étroitement liés aux théories des sciences sociales.
Par exemple, le mécanisme de « pauvreté » ci-dessus est étroitement lié à la théorie de Gurr[^2] selon laquelle les individus se rebellent lorsque les coûts d'opportunité du conflit sont faibles, alors que le mécanisme de « migration » pourrait soutenir une théorie du conflit basée sur les griefs entre groupes sociaux.
Il n'est pas étonnant qu'en apprenant que X cause Y, les chercheurs en sciences sociales demandent immédiatement quel est le mécanisme - ils veulent relier cette découverte à la théorie !

[^2]: Ted Gurr, Why Men Rebel, Princeton University Press, 1970.

Comprendre les mécanismes a non seulement des avantages théoriques mais aussi pratiques.
Premièrement, connaître M permet de deviner pour quelles populations X conduira à Y.
Si le mécanisme du climat/conflit est une réponse physiologique à la chaleur, alors les chocs climatiques peuvent produire des conflits uniquement lorsque la température est assez chaude.
Deuxièmement, connaître M nous aide à considérer d'autres résultats qui peuvent être affectés par X.
Si le mécanisme du climat/conflit est la migration, alors nous pouvons également nous attendre à ce que les chocs climatiques entraînent une surutilisation des biens publics dans les zones urbaines.
Troisièmement, connaître M nous aide à envisager d'autres moyens de provoquer ou d'éviter de provoquer des changements dans Y.
Si le mécanisme du climat/conflit est la pauvreté, alors les programmes de développement pourraient diminuer les conflits en réduisant la sensibilité des revenus aux chocs climatiques, même s'ils ne peuvent pas changer les chocs climatiques.

# Mais il est extrêmement difficile d'identifier les mécanismes causaux car les mécanismes eux-mêmes ne sont pas assignés au hasard…

Prenons un exemple expérimental. Chong et al. (2015)[^3] ont utilisé une expérience de terrain pour étudier l'effet des informations à propos de la corruption sur la participation électorale.
Ils ont assigné au hasard certains bureaux de vote au Mexique pour recevoir des informations sur l'utilisation corrompue des fonds au sein de cette municipalité.
Étonnamment, ils ont découvert que les circonscriptions traitées votaient à des taux inférieurs à ceux des circonscriptions de contrôle.
Ils suggèrent le mécanisme suivant à l'œuvre : les informations sur la corruption convainquent les électeurs que la municipalité est si gravement corrompue que l'élection d'un bon politicien ne la changera pas, de sorte que les individus trouvent que leur vote a moins de valeur.

[^3]: Alberto Chong, Ana L. de la O, Dean Karlan, and Leonard Wantchekon, “Does Corruption Information Inspire the Fight or Quash the Hope? A Field Experiment in Mexico on Voter Turnout, Choice, and Party Identification,” The Journal of Politics 77.1 (2015): 55-71.

En bref, leur argument est :[^4]

[^4] : Notez que Chong et al. testent leur argument en utilisant des données au niveau de l'arrondissement, pas au niveau individuel, mais nous avons adapté leur argument au niveau individuel pour faciliter l'explication.

Réception d'informations sur la corruption (X) $\xrightarrow{+}$ Estime que la corruption est trop grave (M) $\xrightarrow{+}$ Reste à la maison (Y)

Chong et al. font face à un obstacle commun dans l'interprétation de leurs résultats : le mécanisme qu'ils proposent n'a pas été assigné au hasard.
Certaines personnes sont plus enclines à croire que « tous les politiciens sont des vauriens », tandis que d'autres ont tendance à faire pression pour « un changement auquel nous pouvons croire ». Malheureusement, nous ne pouvons observer que le traitement aléatoire qu'un individu a reçu et sa croyance (non aléatoire) au sujet de la corruption ; nous ne pouvons pas dire quelle croyance au sujet de la corruption ils *auraient eu* s'ils avaient reçu l'autre condition de traitement.
Il nous est donc impossible de déterminer, pour chaque individu, dans quelle mesure sa décision d'aller voter a été causée par le mécanisme proposé par rapport à d'autres mécanismes.

Certains chercheurs tentent de contourner ce problème en estimant l'effet *moyen* du traitement sur le mécanisme, puis en estimant l'effet *moyen* du mécanisme sur le résultat.
L'une des raisons pour lesquelles cela est problématique est que nous pouvons imaginer plusieurs facteurs autres que le traitement qui pourraient causer à la fois M et Y.
Supposons que le niveau d'apathie - appelons-le Q - varie parmi les citoyens de notre étude, et Q a un effet très fort sur M et Y.
Les personnes très apathiques pourraient être plus susceptibles de croire que les problèmes sont insolubles, et elles pourraient également être plus susceptibles de rester à la maison le jour du scrutin.
Nous sommes donc susceptibles d'observer une forte corrélation entre M et Y qui est induite par le facteur de confusion de Q, et non par notre traitement X.
Mécaniquement, nos résultats seront biaisés en faveur de la recherche de preuves de l'effet de X sur Y via M simplement parce que Q a produit une relation entre M et Y.

# …et parce que les effets des traitements sont rarement homogènes.

L'autre problème lorsque l'on essaie de décomposer l'effet moyen de X sur M puis de M sur Y est que cette approche suppose que chaque sujet répond au traitement de manière identique.
En rappelant notre exemple dans lequel X est le traitement de l'information, M est la conviction que la corruption est trop sévère, et Y le fait de rester à la maison, nous pouvons imaginer deux types de répondants.
Le type A pensait que la corruption était trop grave pour être résolue jusqu'à ce qu'elle reçoive une lettre contenant des informations sur la corruption dans son district.
Elle fut surprise de voir que le problème n'était pas aussi grave qu'elle l'avait prévu.
Formellement, pour le type A, $M(X=0)=1$ et $M(X=1)=0$, donc X a un effet *négatif* sur M.
Le type B pensait que la corruption était un problème gérable jusqu'à ce qu'elle reçoive une lettre contenant des informations sur la corruption dans son district.
Elle a été surprise par l'ampleur du problème et a abandonné tout espoir de résoudre le problème.
Formellement, pour le type B, $M(X=0)=0$ et $M(X=1)=1$, donc X a un effet *positif* sur M.
Si nous devions faire la moyenne des effets pour ces deux types, nous ne verrions aucune relation entre X et M.

Type | X (traitement de l'info) | M conditionnel à X=0 (non observé) | M conditionnel à X=1 (observé) | Effet de X sur M | Effet de M sur Y | Y (Reste à la maison)
------------- | ------------- | ------------- | ------------- | ------------- | ------------- | -------------
A | 1 | 1 | 0 | négative | négative | 1
B | 1 | 0 | 1 | positive | positive | 1

L'estimation du rôle de M peut être encore plus compliquée lorsque la relation entre M et Y est également hétérogène.
Imaginez que le type A ne vote que lorsqu'elle est en colère (en d'autres termes, M a un effet *négatif* sur Y).
Le type A prévoyait de voter pour exprimer sa colère face à l'omniprésence de la corruption dans son district, même si elle savait que cela n'aurait rien changé, jusqu'à ce qu'elle apprenne que la corruption n'était pas aussi grave qu'elle l'avait imaginé.
Sa passion ardente disparue, elle choisit de rester à la maison le jour des élections.
Cependant, le type B ne vote que lorsqu'elle pense que son vote peut faire la différence (en d'autres termes, M a un effet *positif* sur Y).
Le type B allait voter pour les politiciens non corrompus de sa circonscription jusqu'à ce qu'elle apprenne qu'ils étaient tous corrompus.
Sans aucun espoir de changer la situation, elle a également décidé de rester chez elle le jour du scrutin.
Pour les types A et B, il existe un « effet indirect » de M (en d'autres termes, X affecte Y à M).
Mais nous passerons à côté de cette relation dans l'ensemble car nous ne serons pas en mesure d'obtenir des estimations non biaisées de l'effet moyen de X sur M.[^5]

[^5]: Pour une discussion plus rigoureuse de ces fausses conceptions, voir Adam N. Glynn, “The Product and Difference Fallacies for Indirect Effects,” American Journal of Political Science 56.1 (2012): 257-269.

Nous pouvons imaginer beaucoup plus de « types » que A et B – le but ici est de démontrer intuitivement que parce que M n'est pas assigné au hasard, et parce qu'il est peu probable que les effets de X sur M *et* M sur Y soient identiques pour tout le monde, il sera très difficile de caractériser avec précision l'entremise de M pour notre effet.

# De nombreuses études tentent de décomposer l'effet total d'un traitement entre ses effets « directs » et « indirects ».

Parce que l'apprentissage des mécanismes est riche en promesses théoriques, les chercheurs aimeraient quantifier dans quelle mesure un effet de X sur Y opère via M.
Parfois, les chercheurs essaieront de le faire grâce à une technique appelée « décomposition des effets ».

Une analyse de décomposition des effets tente de décomposer un effet *total* de X sur Y en l'effet que X a sur Y *directement* et l'effet de X sur Y qui se produit *indirectement* via M.
L'« effet total » fait référence à l'effet moyen du traitement (ATE), qui est simplement l'effet moyen de X sur Y.
Toute expérience qui attribue au hasard un traitement afin d'observer ses effets sur certains résultats évalue l'ATE.
Ensuite, le chercheur essaie de quantifier la taille de l'effet que X a sur Y à travers le mécanisme M.
C'est ce qu'on appelle souvent « l'effet indirect » – parce que X affecte indirectement Y via M – ou l'effet moyen causal par médiation (Average Causally Mediated Effect / ACME).
Enfin, le chercheur tentera d'estimer l'effet de X sur Y qui ne passe pas par M.
C'est ce qu'on appelle l'« effet direct » de X sur Y ou l'effet moyen direct contrôlé (Average Controlled Direct Effect / ACDE), car c'est l'effet de X sur Y lorsque nous contrôlons le travail effectué par M.

# Mais soyez prudent lorsque vous utilisez l'analyse de régression pour décomposer les effets.

Bien que couramment utilisée, l'utilisation de l'analyse de régression par médiation présuppose des hypothèses fortes et souvent irréalistes.
Nous utiliserons du code pour illustrer ce que cette méthode implique et démontrer les conditions dans lesquelles elle peut produire des estimations biaisées.

L'idée de base est que si nous avons des données sur le traitement qu'un individu a reçu (X), s'il présente le mécanisme proposé (M) et quel est le résultat (Y), alors nous pouvons distinguer ces effets en utilisant les trois régressions suivantes.

(1) $$ M_i = \alpha_1+aX_i+e_{1i} $$
(2) $$ Y_i = \alpha_2+cX_i+e_{2i} $$
(3) $$ Y_i = \alpha_3+dX_i+bM_i+e_{3i} $$

Comment ferions-nous cela? En utilisant l'équation 1, nous faisons la régression de M sur X pour obtenir l'effet direct de X sur M, qui est le coefficient $a$.
Ensuite, nous passons à l'équation 3, dans laquelle nous faisons la régression de Y sur M et X.
Dans cette régression, le coefficient $b$ représente l'effet direct de M sur Y lorsque nous contrôlons pour X.
Une analyse de décomposition des effets multiplierait $a*b$ pour révéler l'effet indirect de X sur Y via M.
Pour trouver l'effet direct de X sur Y, nous n'avons pas besoin de chercher plus loin que $d$, qui est le coefficient sur X dans l'équation 3 lorsque nous contrôlons pour M.
En d'autres termes, $d$ est l'effet de X sur Y qui ne passe pas par M.
Si l'on additionne l'effet indirect et l'effet direct, on obtient « l'effet total » de X sur Y, qui est égal à $c$.
Pour résumer, l'analyse de décomposition des effets désagrége ostensiblement l'effet total en effet qui est médié via M et l'effet qui n'est pas médié via M, permettant au chercheur de conclure à quel point M est important pour expliquer la relation entre X et Y.[ ^6]

[^6]: Explication adaptée d'Alan Gerber et Donald Green, Field Experiments, W.W. Norton and Company, 2012, chapter 10.

The problem is that this arithmetic only works under some very strong assumptions.
One of these assumptions is that the error terms in regressions 1 and 3 are unrelated to each other—in other words, M can’t be predicted by unobservable factors that also predict Y.
We described this problem intuitively in point 3 when we introduced Q, a confounding variable that contributes both to M and to Y and therefore engenders a very strong relationship between them, even if X’s effect on Y is not operating through M at all.
Now let’s describe this problem using a simulation.

In the following code, we start by creating this Q variable for each individual and defining the “true” effects of X on M, M on Y, and X on Y.
Next, we create hypothetical potential outcomes for M—that is, for each individual, we define what value of M they would reveal if they were treated, and what value of M they would reveal if they were untreated.
These values are related not only to the “true” effect of X on M, but also to Q.
Then we can also define hypothetical potential outcomes for Y.
We do this for four scenarios, all of which assume constant effects, an assumption we will relax later.
Two of these are simple potential outcomes of Y: the Y exhibited by the individual who is untreated and reveals her untreated M potential outcome, and the Y exhibited by the individual who is treated and reveals her treated M potential outcome.
However, we also define two complex potential outcomes of Y: the Y exhibited by the individual who is untreated but reveals her treated M potential outcome, and the Y exhibited by the individual who is treated but reveals her untreated M potential outcomes.
While these potential outcomes bend the mind a bit, they are important to define in the hypothetical so that we can calculate the “true” (but inherently unobservable) direct and indirect effects to compare our decomposition analysis to.

In the second half of the code, we conduct a random assignment of treatment and proceed with the decomposition of effects analysis described above, using the data we “observe.” Under the (strong) assumptions that the error terms are uncorrelated and effects are constant across subjects, $a*b$ = ACME, $d$ = ACDE, and $c$ = ATE.
However, the simulation reveals that $a*b$ > ACME and $d$ < ACDE; that is, we overestimated the average indirect or mediated effect (ACME) and underestimated the average direct effect (ACDE).
Our decomposition of effects analysis was biased because the first assumption – uncorrelated error terms – did not hold: unobserved variable Q predicted both M and Y, and this led us to overestimate the role of the mechanism M.

```{r}
rm(list=ls())

set.seed(20160301)

N <- 1000000

# Simulate Data, Create Potential Outcomes, Estimate "True" Effects -----------------------

# build in an ideosyncratic unobserved characteristic
Q_i <- rnorm(N)

# create the "true model" by defining our treatment effects (tau)
tau_X_on_M <- 0.2 # X's effect on M
tau_M_on_Y <- 0.1 # M's effect on Y
tau_X_on_Y <- 0.5 # total effect of X on Y (ATE), both through M and not through M

# build the potential outcomes (POs) for the mediator
# individual reveals M_1 if treated; M_0 if untreated
# M is a function of both treatment and the unobserved characteristic
M_0 <- 0*tau_X_on_M + Q_i
M_1 <- 1*tau_X_on_M + Q_i

# we can estimate the unbiased Average Treatment Effect (ATE) of X on M
ATE_M <- mean(M_1 - M_0)
ATE_M

# build POs for the outcome variable
Y_M0_X0 <- tau_M_on_Y*(M_0) + tau_X_on_Y*0 + Q_i
Y_M1_X1 <- tau_M_on_Y*(M_1) + tau_X_on_Y*1 + Q_i
Y_M0_X1 <- tau_M_on_Y*(M_0) + tau_X_on_Y*1 + Q_i # this is a "complex" PO
Y_M1_X0 <- tau_M_on_Y*(M_1) + tau_X_on_Y*0 + Q_i # this is a "complex" PO
# some of these POs are "complex" because we are imagining what Y we would
# observe if we assigned treatment but observed the untreated M PO or
# if we assigned control but observed the treated M PO
# building these complex POs is necessary for estimating the "true" direct and indirect effects

# we can estimate the unbiased Average Causally Mediated Effect (ACME)
# we estimate the effects of M holding X constant
# they are the same
# this is the "indirect effect"
ACME_X0 <- mean(Y_M1_X0 - Y_M0_X0)
ACME_X1 <- mean(Y_M1_X1 - Y_M0_X1)
ACME <- mean(((Y_M1_X1 - Y_M0_X1) + (Y_M1_X0 - Y_M0_X0))/2)

# we can estimate the unbiased Average Controlled Direct Effect (ACDE)
# we estimate the effects of X holding M constant
# they are the same
# this is the "direct effect"
ACDE_M0 <- mean(Y_M0_X1 - Y_M0_X0)
ACDE_M1 <- mean(Y_M1_X1 - Y_M1_X0)
ACDE <- mean(((Y_M0_X1 - Y_M0_X0)+(Y_M1_X1 - Y_M1_X0))/2)

# now we build the simple POs for Y
Y_1 <- tau_M_on_Y*(M_1) + tau_X_on_Y*1 + Q_i
Y_0 <- tau_M_on_Y*(M_0) + tau_X_on_Y*0 + Q_i

# we estimate the true ATE of X on Y
# this is the "total effect"
ATE <- mean(Y_1 - Y_0)

ATE
ACDE + ACME # note that the direct and indirect effects sum to the total
ACDE 
ACME
ATE_M

# Random Assignment, Revelation of POs, Attempt to Decompose Effects ---------------------------------

# we assign half of our sample to treatment and half to control
X <- sample(c(rep(1,(N/2)),rep(0,(N/2))))
# we reveal POs for M and Y based on treatment assignment
M <- X*M_1 + (1-X)*M_0
Y <- X*Y_1 + (1-X)*Y_0

model1 <- lm(M ~ X)
a <- coef(model1)[2] # extract the coefficient to get the effect of X on M
a

model2 <- lm(Y ~ X)
c <- coef(model2)[2] # extract the coefficient to get the total effect of X on Y
c

model3 <- lm(Y ~ X + M)
d <- coef(model3)[2] # extract this coefficient to get the effect of X on Y controlling for M
b <- coef(model3)[3] # extract this coefficient to get the effect of M on Y controlling for X

# some would now multiply the average effect of X on M and the average effect of M on Y to get the average indirect/mediated effect of X on Y via M (ACME)
a*b
# but when we compare this to the true ACME, we see that this is biased
ACME

# some would also interpret the average effect of X on Y controlling for M as the average controlled direct effect (ACDE)
d
# but when we compare this to the true ACDE, we see that this is biased
ACDE

# note that we have OVERestimated the average indirect effect and UNDERestimated the average direct effect

# the estimates that are unbiased are the average effects of X on Y and X on M because X is randomly assigned
a
ATE_M
c
ATE
```

Let’s tie this exercise back to the issue raised in point 3. This simulation illustrated that quantifying the mediated effect proves difficult when background predictive variables confound the relationship between M and Y.
Because M is not randomly assigned, it is important for us to think about how likely it is that our M and our Y are both affected by unobserved variables.
In principle, if there are no confounding variables in this relationship, then a decomposition of effects analysis may be unbiased, but this is assumption is strong and usually hard to prove.

While we did not demonstrate it in this simulation, it is also possible to show that decomposition of effects also breaks down when treatment effects are heterogeneous (we introduced the intuition for this in point 4).
The technical reason for this comes from our law of expectations, which is: $E[a*b]$ = $E[a]E[b]+cov(a,b)$.
If we have constant treatment effects, then $a$ and $b$ do not covary, the covariance term drops out, and we can simply multiply $a*b$ to get the ACME.
However, if the covariance term is non-zero, then we are not able to estimate this indirect effect from these two coefficients obtained from separate regressions.
We constructed constant treatment effects in order to be able to demonstrate the process of decomposing effects, but if we were to re-do the simulation with heterogeneous treatment effects that covary, then we would not even be able to calculate the ACME or ACDE using the potential outcomes approach at the beginning of our code.

**What you can do...** Before you embark on a decomposition of effects analysis, ask yourself:

- Can I imagine any unobserved variables that predict both M and Y?
- Is it possible that my subjects respond to treatment effects in different ways?

If the answer to any of these questions is yes, we strongly recommend that you proceed with caution.
In particular, think carefully about how unobserved variables and heterogeneous treatment effects would affect your estimation strategy.

# Sometimes subgroup analysis can provide suggestive evidence for or against a mechanism.


In points 3-6, we’ve cautioned researchers against trying to confidently quantify the proportion of an effect that is mediated by a particular mechanism, but there may be other ways to learn more about mechanisms at work in a particular study.
In point 1, we underscored the tight relationship between mechanisms and theory.
Just because it’s challenging to quantify evidence of a mechanism directly does not mean we cannot explore the testable predictions of the theory in which our mechanism is featured!

One strategy is to use subgroup analysis, or treatment-by-covariate interactions, to see whether different populations respond to the treatment differently in accordance with our theories.
For example, suppose we wanted to learn more about the role of income in mediating the climate/conflict relationship.
One of the testable implications of a theory in which income plays a mediating role is that we would expect climate shocks to be associated with conflict in areas where income is sensitive to climate shocks but not where income is independent of climate shocks.
Sarsons (2015)[^7] does exactly this.
Exploiting the fact that districts downstream of irrigation dams do not depend on rainfall for income while districts upstream of irrigation dams do, she explores the income mechanism by testing whether rain shocks predict riot incidence in downstream districts but not in upstream districts.
Formally, she tests these hypotheses:

[^7]: Heather Sarsons, “Rainfall and Conflict: A Cautionary Tale.” Journal of Development Economics 115 (2015): 62-72.

- X$\rightarrow$Y in places where X is known to affect M [Rainfall shocks will increase riots in areas where rainfall shocks will negatively affect income (upstream of dam).] 
- X has no effect on Y in places where X has no effect on M [Rainfall shocks will have no effect on riots in areas where income is not sensitive to rainfall (downstream of dam).]

However, she found that the relationship between rainfall shocks and riot incidence held just as tightly in the downstream districts where income was not sensitive to rainfall.
She interprets this finding as “suggestive” evidence *against* the income mechanism.
To be clear, Sarsons did not conduct any mediation analysis: she did not measure the income of each village and quantify the direct effect of rainfall shocks on riots and the indirect effect of rainfall shocks on riots through income.
Instead, she looked for the heterogeneous treatment effects the theory would have implied and, finding no evidence of them, concluded that the income channel may be less important than previously thought.

**What you can do...** In future projects, ask yourself: If the mechanism is M, which groups or units would I expect to exhibit a treatment effect, and which groups or units would I expect not to respond to treatment? Next, test whether these predictions are supported by your data and interpret this as suggestive evidence for or against your proposed mechanism M.
Keep in mind that such evidence is not decisive because the groups could differ in other ways that could affect their responsiveness to the treatment.

#  We can also look for suggestive evidence by looking at the effects of our treatment on various outcomes.

Again, while it’s difficult to quantify evidence of a mechanism at work, we can always explore the testable implications of the theory in which our mechanism features.
In point 7, we did this by exploring whether the treatment affected the particular subgroups for which a treatment effect is implied by our theory.
Another approach is to explore whether the treatment affects only the outcomes implied by our theory.

For example, many social scientists are interested in how mass education influences democracy.
Several theories of democratization expect different mechanisms would connect education and democracy.
First, according to modernization theory, education could facilitate the smooth functioning of democracy by undermining group attachments (such as ethnicity or religion) in favor of merit.[^8]  Second, according to social theorists of oppression, education could undermine democracy by reinforcing obedience to authority, which is inherent in a classroom structure.[^9]  Third, according to many political scientists and psychologists, education can encourage democratic participation by empowering individuals with the ability to acquire and act on knowledge.[^10]  Friedman et al. (2011)[^11]  decide to tease apart these mechanisms by investigating the results of a field experiment in which Kenyan girls were randomly assigned to receive an education subsidy.
 They followed up with the students five years after the program and asked them several questions designed to test which of these three mechanisms were at work: Did the girls accept a husband’s right to beat his wife? Was a parent involved in selecting their spouse? How strongly did the girl identify with her religious or ethnic group? Did the girl regularly read news?

[^8]: Marion Joseph Levy, Modernization and the Structure of Societies, Princeton University Press, 1966.
[^9]: Frantz Fanon, The Wretched of the Earth, Grove Press, 1964. John Lott, Jr., “Public Schooling, Indoctrination and Totalitarianism,” Journal of Political Economy 107(6), 1999. 
[^10]: Gabriel Almond and Sidney Verba, The Civic Culture: Political Attitudes and Democracy in Five Nations, Sage Publications, 1963. Robert Mattes and Michael Bratton, “Learning about Democracy in Africa: Awareness, Performance, and Experience,” American Journal of Political Science, 51(1), 2007.
[^11]: Willa Friedman, Michael Kremer, Edward Miguel, and Rebecca Thornton, “Education as Liberation?” NBER Working Paper 16939, 2011.

The following table outlines the direction of the effects that each theory would suggest.
Note that the various mechanisms being tested here result from theories with diverging predictions on some of these outcomes.
The predictions from each of the three mechanisms are outlined on the rows, followed by the actual results.
We can see that two of the outcomes collected provided support for modernization theory.
However, modernization theory would have predicted a decrease in religious or ethnic group association (in reality, there was no effect) and had no predictions for newspaper readership (in reality, readership increased).
None of the predictions of the obedience to authority mechanism were supported by the data.
However, the data supported all four of the predictions of individual empowerment theory.
The authors conclude that it is more likely that $X→M3→Y$ than $X→M1,M2→Y$.[^12]

[^12]: In the actual study, the authors were surprised to uncover evidence that education also increased individuals’ acceptance of political violence.
While they still argue that individual empowerment is responsible for the relationship between education and democracy, they caution that education does not always lead to democratization (that is, M3→Y but it is also possible that M3→NOT Y).
Nonetheless, their approach is a useful demonstration of how multiple outcomes may shed light on mechanisms.

Mechanism | Acceptance of husband's right to beat wives (Y1) | Parent involved in selecting spouse (Y2) | Association with religion, ethnic identity (Y3) | Reads news (Y4)
------------- | ------------- | ------------- | ------------- | ------------- 
(M1) Modernization | $\downarrow$ | $\downarrow$ | $\downarrow$ | No effect 
(M2) Obedience to authority | $\uparrow$ | $\uparrow$ | $\uparrow$ | No effect 
(M3) Individual empowerment | $\downarrow$ | $\downarrow$ | No effect | $\uparrow$
Actual effect | $\downarrow$ | $\downarrow$ | No effect | $\uparrow$ 

This study, like Sarsons’s study, is not trying to quantify how much of the effect of X on Y is conveyed via M.
However, through thoughtful investigation of various outcomes, the authors are able to provide suggestive evidence of which mechanisms seem most plausible.

**What you can do...** In future projects, ask yourself: If the mechanism is M, which outcomes would I expect to be affected by my treatment, and which outcomes would I expect not to be affected by my treatment? Next, test whether these predictions are supported by your data and interpret this as suggestive evidence for or against your proposed mechanism M.

#  Designing complex treatments can help narrow our understanding of what part of the treatment is “doing the work.”

Sometimes experimental researchers will try to better understand mechanisms by adding or subtracting elements of the treatment that are thought to trigger different mechanisms.
This approach is sometimes called “implicit mediation analysis” because different components are X are thought to *implicitly* manipulate certain mechanisms.
This, of course, is an assumption: because we are not measuring M directly, we are relying on a theoretical claim that component A will trigger M, whereas component B will not.

For example, many governments including Mexico, Brazil, Tanzania, and Uganda have created conditional cash transfer programs to address poverty.
These programs provide cash to poor individuals, but they frequently come with conditions such as attending school or a job training program.
Until recently, we knew only that these programs (X) successfully reduced poverty (Y) and that X caused Y either via cash or via the required attendance at school or job programs.
To distinguish between these mechanisms, Baird et al. (2011)[^13] conducted an experiment in Malawi, where they assigned one group of families to receive a *conditional* cash transfer for the regular school attendance of their girls, another group of families to receive the cash *unconditionally*, and a control group to receive no transfer.
This design “implicitly” manipulated M: while girls in the unconditional transfer group could also seek out education, school attendance (the condition under study) would likely be higher in the group that was required to seek it out.
Unsurprisingly, school attendance and test performance was better for the group receiving conditional cash transfers.
However, their measures of Y--the rate at which the girls became pregnant or married--were actually better (lower) in the group receiving the unconditional cash transfers.
The authors concluded that attendance requirements associated with conditional cash transfers were not probably not the mechanism responsible for the success of these programs in reducing the symptoms of poverty.

[^13]: Sarah Baird, Craig McIntosh, Berk Ozler, “Cash or Condition? Evidence from a Cash Transfer Experiment,” Quarterly Journal of Economics 126, 2011.

Studies like these help not only social scientists to learn more about the channels through which X causes Y, but also policymakers to explore and discover new treatments.
After several other studies joined Baird et al.
in demonstrating the remarkable effects of unconditional cash transfers, many governments and organizations have begun to implement unconditional cash transfer programs.

**What you can do...** In future projects, ask yourself: Can my treatment be “unpacked” into multiple treatment arms, some that implicitly manipulate M, and some that do not? Consider using a factorial design to identify the effects of different treatment arms.
If you have ample power, comparing the various treatment arms will provide you with suggestive evidence for or against M.

#  Despite the difficulties in empirically measuring mechanisms, it is worth paying serious attention to them but being cautious in our language.

Attempting to identify causal mechanisms is a noble endeavor.
Articulating causal mechanisms is what allows us to unpack “black box” treatments and understand why and how certain treatments work.
Even though causal claims can be (and often are) made without evidence for a causal mechanism, exploring causal mechanisms is what enables us to extend the research frontier and re-evaluate how our evidence maps on to our theories.
For these reasons, audiences (be they the general public or academic reviewers) are often understandably eager for you to expound upon causal mechanisms after demonstrating evidence for a provocative causal claim.
In anticipation of this, it is worth considering whether it is possible to design a way to test causal mechanisms in advance of implementing an experiment.
If not, consider whether certain outcome measures or treatment by covariate interactions would provide some support for a particular causal mechanism, and be explicit about the limitations of this kind of analysis in your write-up.
Mechanisms are an exciting domain of inquiry and should be considered both in the design and analysis of an experiment, but we should be sure to discuss mechanisms with caution appropriate to our ability to identify a particular mechanism and avoid overselling the argument.
