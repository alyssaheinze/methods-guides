---
bibliography: ../../Research-Group-Bibliography/big.bib
output: 
  bookdown::html_document2:
    number_sections: true
    toc: true
    theme: journal
    extensions: +implicit_figure
    code_folding: hide
    fig_caption: true
    pandoc_args: [
           --filter, pandoc-crossref
        ]
---

<!-- title: "10 choses à savoir sur les tests d'hypothèses" -->
<!-- author: "Auteur du guide des méthodes: Jake Bowers" -->

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}

<style>
.comment{
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
color: black;
}
</style>

```{r echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
require(knitr)
opts_chunk$set(strip.white=TRUE,
               width.cutoff=132,
               size='\\scriptsize',
               out.width='.9\\textwidth',
               message=FALSE,
               warning=FALSE,
               echo=TRUE,
               comment=NA,
               tidy='styler',
               prompt=FALSE,
               results='markup')

library(randomizr)
library(coin)
```

# Les tests d'hypothèse résument les informations dans les designs de recherche pour aider les gens à raisonner sur les effets du traitement

Lorsque les chercheurs signalent que « l'effet moyen estimé du traitement est de 5 (p=0,02 $) », ils utilisent un raccourci pour dire :
"Cher lecteur, au cas où vous vous demanderiez si nous pourrions distinguer le signal du bruit dans cette expérience en utilisant des moyennes, en fait, nous le pouvons.
Les résultats expérimentaux ne sont **pas cohérents** avec l'idée que le traitement n'a eu aucun effet."
Les gens utilisent des tests d'hypothèses dans des études d'observation ainsi que dans des expériences randomisées.
Ce guide se concentre sur leur utilisation dans des expériences randomisées ou des designs de recherche qui tentent d'organiser les données
de sorte qu'elles paraissent « aléatoires » (comme les designs de régression sur discontinuité ou d'autres designs naturels ou quasi expérimentaux).

La valeur $p$ résume la capacité d'un test donné à distinguer le signal du bruit.
Comme nous l'avons vu dans [10 choses à savoir sur la puissance statistique] (https://egap.org/resource/10-things-to-know-about-statistical-power),
si une expérience peut détecter un effet de traitement dépend non seulement de la taille du pool expérimental, mais aussi de la distribution du résultat
^[Les résultats avec de grandes valeurs aberrantes ajoutent du bruit ; les résultats qui sont pour la plupart à 0 ont peu de signal ; un découpage par bloc, une pré-stratification ou un ajustement de covariance peuvent réduire le bruit.], la distribution du traitement et la force substantielle de l'intervention elle-même.
Lorsqu'une chercheuse calcule une $p$-valeur à la suite d'un test d'hypothèse, elle résume tous ces aspects du design de recherche en fonction d'une affirmation particulière.
---généralement une allégation selon laquelle le traitement n'a eu aucun effet causal.

Le reste de ce guide explique les étapes d'un test d'hypothèse :
partir de l'hypothèse nulle (l'affirmation selon laquelle le traitement n'a eu aucun effet causal),
vers des statistiques de test résumant les données observées (comme une différence de moyennes),
jusqu'à la création d'une distribution de probabilité qui permet le calcul d'une $p$-valeur.

Il aborde également l'idée de rejeter (mais ne pas accepter) une hypothèse et aborde la question de ce qui fait un bon test d'hypothèse
(indice : un test d'hypothèse idéal devrait rarement mettre en doute la vérité et distinguer même les signaux faibles du bruit).
Voir également [10 choses à savoir sur l'inférence de randomisation] (https://egap.org/resource/10-things-to-know-about-randomization-inference) pour plus de détails sur ces idées.

# Dans une expérience, une hypothèse est une affirmation sur des relations causales non observées

Nous faisons des expériences pour faire des comparaisons causales interprétables [@kinder1993behalf], et nous estimons souvent les effets causaux moyens.
Qu'est-ce que le test d'hypothèse a à voir avec l'inférence causale?
Dans cette section, nous expliquons la distinction entre :
évaluer des postulats sur les effets causaux et faire les meilleures suppositions sur les effets causaux.

## Un aperçu rapide du problème fondamental de l'inférence causale et une introduction à quelques notations

Dans [10 choses à savoir sur l'inférence causale](https://egap.org/resource/10-things-to-know-about-causal-inference), souvenez-vous que la conceptualisation contrefactuelle de la causalité utilise l'idée de résultats potentiels pour **définir** la cause et formaliser ce que nous **entendons** lorsque nous disons « X cause Y » ou « Fumer cause le cancer » ou « L'information augmente la conformité fiscale ».

Bien qu'il existe d'autres façons de penser la causalité (@brady2008causation), l'approche contrefactuelle suggère que nous imaginons que chaque personne, $i$, paierait ses impôts, $y_{i}$,
si on lui donne des informations sur l'utilisation qui est faite de ces taxes.
Écrivez $Z_i=1$ pour signifier que l'information était donnée à la personne et $Z_i=0$ si aucune information n'a été donnée afin que nous puissions écrire $y_{i,Z_i=1}$ pour désigner le montant des taxes payées par quelqu'un a donné des informations et $y_{i,Z_i=0}$ pour désigner le montant des impôts payés par quelqu'un qui n'a pas donné d'informations en particulier.
Dans une expérience réelle, nous pourrions [randomiser la fourniture d'informations aux citoyens](http://egap.org/metaketa/metaketa-ii-taxation), afin que certaines personnes aient l'information et d'autres pas.

On observe les impôts payés par les gens dans les deux conditions mais, pour une même personne, on ne peut observer que les impôts qu'ils payent dans l'une des deux conditions.
Qu'est-ce que **signifie** lorsque nous disons « effet causal » ?
Cela signifie souvent que le résultat dans une condition ($y_{i,Z_i=1}$ s'écrit un peu plus simplement $y_{i,1}$) et le résultat dans l'autre condition ($y_{i,Z_i=0 }$ ou $y_{i,0}$) *différent* pour une personne donnée, de sorte que nous écririons $y_{i,Z_i=1} \ne y_{i,Z_i=0}$.

Nous ne pouvons pas observer *à la fois* $y_{i,1}$ et $y_{i,0}$ pour chaque personne
--- si nous avons donné des informations sur les impôts à une personne nous observons $y_{i,1}$ et donc nous ne pouvons pas observer comment ils auraient agi s'ils n'avaient pas reçu cette information ($y_{i,0}$).
Ainsi, nous ne pouvons pas utiliser l'observation directe pour en savoir plus sur cet effet causal contrefactuel et nous ne pouvons que **déduire** à ce sujet.
@holland1986statistics appelle cette incapacité à utiliser l'observation directe pour en savoir plus sur la causalité contrefactuelle le "problème fondamental de l'inférence causale".

## Un aperçu des approches basées sur l'estimation de l'inférence causale dans les expériences randomisées.

Les sciences statistiques ont abordé ce problème de trois manières principales.
C'est-à-dire, lorsqu'on lui a demandé : « Est-ce que l'information amène les gens à payer leurs impôts ? » nous avons tendance à dire : « Nous ne pouvons pas répondre directement à cette question.
Cependant, nous pouvons répondre à une question connexe."
[Dix types d'effet de traitement que vous devriez connaître](https://egap.org/resource/10-types-treatment-effect-you-should-know-about) décrit une idée que nous créditons à Jerzy Neyman où un scientifique peut **estimer les effets causaux moyens** dans une expérience randomisée même si les effets causaux individuels ne sont pas observables.
Le travail de Judea Pearl sur l'estimation de la probabilité conditionnelle d'un résultat basé sur un modèle causal de ce résultat est similaire à cette idée, où l'accent est mis sur les probabilités conditionnelles des $y_{i}$.
C'est-à-dire que ces deux approches répondent à la question causale fondamentale en dirigeant la question vers les moyennes ou les probabilités conditionnelles.
Une approche connexe, de Don Rubin commence par **prédire** les résultats potentiels au niveau individuel en utilisant des informations de base et un modèle de probabilité de $Z_i$ (tel que, disons, $Z \sim \text{Bernoulli}(\pi)$ ) et un modèle de probabilité des deux résultats potentiels tels que, disons,
$(y_{i,1},y_{i,0}) \sim \text{Multivariate Normal}(\bbeta \bX, \bSigma)$ avec un vecteur de coefficients $\bbeta$, une matrice $n par p$ de variables $\bX$ (contenant à la fois l'assignation du traitement et d'autres variables et une matrice $p par p$ de variance-covariance $\Sigma$ décrivant comment toutes les colonnes de $\bX$ sont liées).

The second general approach starts with such probability models relating treatment, other variables, and outcomes to each other, and combines them using Bayes Rule to produce posterior distributions for quantities like the individual level treatment effects or average treatment effects (see [@imbens2007causal] for more on what they call the Bayesian Predictive approach to causal inference).
So, the predictive approach changes the fundamental question from one about averages to one that focuses on differences in predicted potential outcomes for each person
(although mostly these individual predicted differences are summarized using characteristics of the posterior distributions implied by the probability models and the data like the average of the predictions.)

La deuxième approche générale commence avec les mêmes modèles de probabilité reliant le traitement, d'autres variables et les résultats.
 Ensuite, à l'aide du théorème de Bayes, elle les combine pour produire des distributions postérieures pour des quantités telles que l'effet du traitement au niveau individuel ou l'effet moyen du traitement (voir [@imbens2007causal] pour plus de détails sur l'approche prédictive bayésienne de l'inférence causale).
Ainsi, l'approche prédictive change la question fondamentale : on ne se focalise plus sur les moyennes, on se concentre sur les différences dans les résultats potentiels prévus pour chaque personne.
(bien que la plupart de ces différences prédites individuelles soient résumées en utilisant les caractéristiques des distributions postérieures impliquées par des modèles de probabilité et des données comme la moyenne des prédictions.)

## Le test d'hypothèse est une approche statistique du problème fondamental de l'inférence causale en faisant des affirmations sur l'inobservé.

La troisième approche de ce problème amène une nouvelle question.
@fisher:1935[Chapitre 2], nous apprenons que nous pouvons poser la question fondamentale de savoir s'il existe un effet causal pour une seule personne, mais la réponse ne peut être qu'en termes de quantité d'information fournie par le design de recherche et les données.
C'est-à-dire que l'on peut émettre l'hypothèse que, pour la personne $i$, l'information n'a fait aucune différence pour son résultat, de sorte que $y_{i,1}=y_{i,0}$ ou $y_{i,1}=y_ {i,0}+\tau_i$ où $\tau_i=0$ est pour tout le monde.
Cependant, la réponse à cette question doit être quelque chose comme "Ce design de recherche et cet ensemble de données fournissent beaucoup d'informations sur ce modèle, cette idée ou cette hypothèse." ou, comme ci-dessus, "Ce design de recherche n'est pas cohérent avec cette affirmation." (Voir @rosenbaum2002(Chapitre 2), @rosenbaum2010(chapitre 2) et @rosenbaum2017observation, pour plus de détails sur cette approche.)

# L'hypothèse nulle d'absence d'effet de traitement est une déclaration précise sur les résultats potentiels

Même si nous ne pouvons pas utiliser l'observation directe pour en savoir plus sur les effets causaux contrefactuels, nous pouvons toujours poser des questions à leur sujet ou créer des modèles théoriques qui lient ensemble une intervention (ou un traitement), des caractéristiques de base et des résultats potentiels.
Le modèle le plus simple de ce type indique que le résultat sous traitement serait le même que le résultat sous contrôle pour toutes les unités ; c'est-à-dire que, quelles que soient les caractéristiques de base ou les informations fournies dans les conditions de traitement, chaque personne paierait le même montant d'impôts : $y_{i,1}=y_{i,0}$ pour toutes les unités $ je$.
Pour souligner la nature provisoire et théorique de ce modèle, les gens ont appelé cela une **hypothèse**, l'écrivent souvent comme "l'hypothèse nulle forte". et utilisez le raccourci suivant : $H_0 :
y_{i,1}-y{i,0}=\tau_i$ où $\tau_i=0$ est pour toutes les unités $i$.

**Side Note:** Réfléchir à l'hypothèse nulle forte nous fait réaliser que nous pourrions créer **d'autres modèles reliant $y_{i,1}$ et $y_{i,0}$** dans lesquels les résultats potentiels se rapportent de manière non additive ou linéaire, et où l'effet n'a pas besoin d'être nul ou identique pour toutes les unités : par exemple,
nous pourrions émettre l'hypothèse que $\tau_i=\{5,0,-2\}$ 5 pour l'unité 1, 0 pour l'unité 2 et -2 pour l'unité 3 dans une expérience de 3 unités.

Notez également que cette manière d'écrire les résultats potentiels, avec le résultat potentiel pour l'unité $i$ *se référant uniquement à $i$* et non à d'autres unités ($y_{i,Z_i}$), *fait partie du modèle*.
C'est-à-dire que le modèle de $H_0 : y_{i,1}=y_{i,0}$ implique que le traitement n'a d'effet sur personne --- et aucun effet signifie aussi aucun effet de débordement.

Nous pourrions être un peu plus précis en écrivant les résultats potentiels comme suit : le résultat potentiel de l'unité $i$ lorsqu'elle est assignée au traitement et lorsque toutes les autres unités sont assignées à un autre ensemble de traitements $\bZ_{~i}=\{Z_j,Z_k,\ldots \}$ peut s'écrire $y_{i,Z_i=1,\bZ_{~i}}$.

Voir @bowers2013reasoning et @bowers2018models pour en savoir plus sur l'idée qu'une hypothèse est un modèle théorique qui peut être testé avec des données dans le contexte d'hypothèses sur la propagation des effets de traitement à travers un réseau.

# L'hypothèse nulle d'absence d'effet de traitement est une déclaration sur les résultats potentiels agrégés

Une expérience peut influencer certaines unités mais n'avoir, en moyenne, aucun effet.
Pour codifier cette intuition, les chercheurs peuvent écrire une hypothèse nulle sur une *moyenne* de résultats potentiels, ou un autre agrégat de résultats potentiels, plutôt que sur l'ensemble des résultats potentiels.

Parce que la plupart des discussions actuelles sur les effets causaux parlent de la *moyenne* des effets, les gens écrivent l'hypothèse nulle faible ainsi $H_0 :
\bar{\tau}=0$ où $\bar{\tau}=(1/N)\sum_{i=1}^N \tau_i$.
Encore une fois, l'hypothèse est une déclaration ou un modèle d'une relation entre des résultats potentiels partiellement observés.
Mais, ici, il s'agit de leur moyenne.
On pourrait, en principe, articuler des hypothèses sur d'autres agrégats : médianes, centiles, ratios, moyennes tronquées, etc.
Cependant, faire des hypothèses sur l'effet moyen nous simplifie la tâche : nous connaissons les propriétés de la moyenne d'observations indépendantes à mesure que la taille de l'échantillon augmente, de sorte que nous pouvons faire appel au théorème central limite pour décrire la distribution de la moyenne pour de grands échantillons --- et ceci, à son tour, rend le calcul des $p$-valeurs rapide et facile pour de grands échantillons.

# La randomisation nous permet d'utiliser ce que nous observons pour tester des hypothèses sur ce que nous n'observons pas.

Que l'on émette des hypothèses sur les effets au niveau unitaire directement ou sur leurs moyennes, nous devons encore faire face au problème de la distinction entre le signal et le bruit.
Une hypothèse se réfère uniquement aux résultats potentiels.
Ci-dessus, en supposant aucune interaction entre les unités, nous avons imaginé deux résultats potentiels par personne, mais nous n'en observons qu'un par personne.
**Comment pouvons-nous utiliser ce que nous observons pour en savoir plus sur les modèles théoriques de quantités partiellement observées ?**
Dans cette expérience simple, nous savons que nous observons l'un des deux résultats potentiels par personne, selon le traitement qui lui a été attribué.
Ainsi, nous pouvons lier les résultats contrefactuels non observés à un résultat observé ($Y_i$) en utilisant l'assignation de traitement ($Z_i$) comme suit :

$$ Y_i = Z_i y_{i,1} + (1 - Z_i) y_{i,0} $$ {#eq:identity}

@eq:identity signifie que notre résultat observé, $Y_i$ (ici, le montant des impôts payés par la personne $i$), est $y_{i,1}$ lorsque la personne est assignée au groupe de traitement ($Z_i=1$), et $y_{i,0}$ lorsque la personne est assignée au groupe de contrôle.

**Combien d'informations notre design de recherche et nos données contiennent-ils sur l'hypothèse ?**
Imaginez, pour l'instant, l'hypothèse selon laquelle le traitement ajoute 5 % au paiement des impôts de chaque personne de telle sorte que $H_0 : y_{i,1} = y_ {i,0} + \tau_i$ où $\tau_i=5$ pour tout $i$.

Continuons pour les besoins de l'argumentation.
Qu'impliquerait cette hypothèse pour ce que nous observons ? Nous avons l'équation reliant l'observé à l'inobservé dans @eq:identity donc, ce modèle ou cette hypothèse impliquerait que :

$$ \begin{aligned}  Y_i & = Z_i ( y_{i,0} + \tau_i ) + ( 1 - Z_i) y_{i,0} \\
& = Z_i  y_{i,0} + Z_i \tau_i + y_{i,0} - Z_i y_{i,0} \\
& = Z_i \tau_i  + y_{i,0}
\end{aligned}
$$

Ce que nous observons, $Y_i$, serait soit $y_{i,0}$ dans la condition de contrôle, $Z_i=0$ ou $\tau_i + y_{i,0}$ (ce qui serait $5 + y_{i ,0}$ dans la condition de traitement).

Cette hypothèse implique en outre que $y_{i,0} = Y_i - Z_i \tau_i$ ou $y_{i,0} = Y_i - Z_i 5$.
Si nous soustrayons 5 de chaque réponse observée dans la condition de traitement, alors notre hypothèse implique que nous observerions $y_{i,0}$ pour tout le monde.
Autrement dit, en soustrayant 5, nous rendrions le groupe de contrôle et le groupe de traitement équivalents en termes de résultats observés.
**Cette logique nous donne une implication observable de l'hypothèse.**

**The sharp null hypothesis of no effects** specifies that $\tau_i=0$ for all $i$.
And this in turn implies that $y_{i,0} = Y_i - Z_i \tau_i = Y_i$.
That is, it implies that what we observe, $Y_i$, is what we would observe if every unit were assigned to the control condition.
And the implication then, is that we should see no differences between the treated and control groups in their observable outcomes.

**The weak null hypothesis of no effects** specifies that $\bar{\tau}=\bar{y}_{1} - \bar{y}_0 = 0$, and we can write a similar identity linking means of unobserved potential outcomes to means of observed outcomes in different treatment conditions.


# Test statistics summarize the relationship between observed outcomes and treatment assignment.

Given a hypothesis and a mapping from unobserved to observed outcomes, the next ingredient in a hypothesis test is a test statistic.
A test statistic summarizes the relationship between treatment and observed outcomes using a single number.
In general, we would like our test statistics to take on larger values the larger the treatment effect.
The code below, for example, shows are two such test statistics using an example experiment with 10 units randomized into two groups (you can press the "CODE" button to see the R code).


```{r setupdata, results='hide', echo=TRUE}
## First, create some data, 
##  y0 is potential outcome to control
N <- 10
y0 <- c(0,0,0,0,1,1,4,5,400,500)
## Different individual level treatment effects
##tau <- c(1,3,2,10,1,2,3,5,1,1)*sd(y0)
set.seed(12345)
tau <- round(rnorm(N,mean=sd(y0),sd=2*(sd(y0)))) ##c(10,30,200,90,10,20,30,40,90,20)
tau <- tau*(tau>0)
## y1 is potential outcome to treatment
y1 <- y0 + tau
#sd(y0)
#mean(y1)-mean(y0)
# mean(tau)
## Z is treatment assignment
set.seed(12345)
Z <- complete_ra(N)
## Y is observed outcomes
Y <- Z*y1 + (1-Z)*y0
## The data
dat <- data.frame(Y=Y,Z=Z,y0=y0,tau=tau,y1=y1)
#(mean(y1) - mean(y0))/sd(y0)
#dat
#pvalue(oneway_test(Y~factor(Z),data=dat,distribution=exact(),alternative="less"))
#pvalue(wilcox_test(Y~factor(Z),data=dat,distribution=exact(),alternative="less"))
#pvalue(oneway_test(Y~factor(Z),data=dat,distribution=exact(),alternative="greater"))
#pvalue(wilcox_test(Y~factor(Z),data=dat,distribution=exact(),alternative="greater"))
## The mean difference test statistic
meanTZ <- function(ys,z){ 
	mean(ys[z==1]) - mean(ys[z==0])
}

## The difference of mean ranks test statistic
meanrankTZ <- function(ys,z){
	ranky <- rank(ys)
	mean(ranky[z==1]) - mean(ranky[z==0])
}

observedMeanTZ <- meanTZ(ys=Y,z=Z)
observedMeanRankTZ <- meanrankTZ(ys=Y,z=Z)
```

The first test statistic is the mean difference (`meanTZ`) and returns an observed value of `r observedMeanTZ` and the second is the mean difference of the rank-transformed outcomes (`meanrankTZ`), which returns a value of `r signif(observedMeanRankTZ,2)`.
One could also use versions of these test statistics standardized by their estimated standard error (see @chung2013exact for an argument in favor of this test statistic).
To test the sharp null hypothesis of no effects, one may choose almost any test statistic such that the values of that function increase as the difference between treated and control outcomes increase (see @rosenbaum:2002, Chapter 2, for a discussion of "effect increasing" test statistics).

Test of the weak null of no effects use the differences of means (perhaps standardized or studentized) as the test statistic.


# $p$-values encode how much information a research design and test statistic tell us about the hypothesis.
Hypothesis tests require distributions of the test statistic under the hypothesis.

Given a claim about the possible results of the experiment (i.e. an hypothesis) and a way to summarize the observed data as it bears on the hypothesis (i.e. a test statistic that should get bigger as the results diverge from the hypothesis as explained above), we now want to move beyond description of the observed data to learn how much natural variability we would expect to see in the test statistic given the research design entertaining the hypothesis (to get us back to the question of signal and noise).

How much evidence we have about a hypothesis depends on the design of the study.
A large experiment, for example, should have more information about a hypothesis than a small one.
So, what do we mean by evidence against the hypothesis? How would we formalize or summarize this evidence so that larger experiments tend to reveal more and small experiments tend to reveal less information?

One answer to this question is to refer to the thought experiment of repeating the study.
Imagine, for the sake of argument, that the hypothesis was correct.
If we repeated the study and calculated the test statistic we would receive a number --- this number would reflect the outcome of the experiment *under the hypothesis*.
Now, imagine repeating the hypothetical experiment many times, recalculating the test statistic each time.
The distribution of the test statistics would then tell us all of the test statistics that could have occurred if the null hypothesis were true.
If the test statistic is a sum or mean, then in a large experiment, we know that the distribution of those numbers will be more closely concentrated around the focal hypothesized value (say, $t(Z,y_0)$) than in a small experiment.

When we compare what we actually observe, $t(z,Y)$, to the distribution of what we could have observed under the null, we learn that our given study is typical or not typical of the null hypothesis.
And we encode this typicality or extremity with a $p$-value.


Notice that the $p$-value does not tell us about the probability associated with the observed data.
The observed data is observed.
The probability arises from the hypothetical, but possible, repetition of the experiment itself, the test statistic, and the hypothesis.
The one-tailed $p$-value is the probability of seeing a value of our test statistic as great or greater than we actually observed considering, for the sake of argument, a given hypothesis.

## An example: Testing the Sharp Null Hypothesis of No Effects

Let us test the sharp null hypothesis of no effects.
In the case of the example experiment, the treatment was assigned to exactly 5 observations out of 10 completely at random.
To repeat that operation, we need only permute or shuffle the given $Z$ vector (you can see the Code by clicking on the "Code" button).

```{r}
repeatExperiment <- function(Z){
	sample(Z)
}
```

We already know that $H_0: y_{i,1} = y_{i,0}$ implies that $Y_i=y_{i,0}$.
So, we can describe all of the ways that the experiment would work out under this null by simply repeating the experiment (i.e. re-assigning treatment) and recalculating a test statistic each time.
The following code repeatedly re-assigns treatment following the design and calculates the test statistic each time.

```{r gendists, cache=FALSE}
set.seed(123457)
possibleMeanDiffsH0 <- replicate(10000,meanTZ(ys=Y,z=repeatExperiment(Z=Z)))
set.seed(123457)
possibleMeanRankDiffsH0 <- replicate(10000,meanrankTZ(ys=Y,z=repeatExperiment(Z=Z)))
```

And these plots show the distributions of the two different test statistics that would emerge from the world of the null hypothesis (the curves and short ticks at the bottom of the plots).
The plots also show the observed values for the test statistics that we can use to compare what we observe (the long thick lines) with what we hypothesize (the distributions).


```{r calcpvalues, results='hide'}
pMeanTZ <- min( mean( possibleMeanDiffsH0 >= observedMeanTZ ),
	       mean( possibleMeanDiffsH0 >= observedMeanTZ ))

pMeanRankTZ <- min( mean( possibleMeanRankDiffsH0 >= observedMeanRankTZ ),
		    mean( possibleMeanRankDiffsH0 <= observedMeanRankTZ ))
pMeanTZ
pMeanRankTZ
```


```{r fig.cap="An example of using the design of the experiment to test a hypothesis."}
par(mfrow=c(1,2),mgp=c(1.5,.5,0),mar=c(3,3,0,0),oma=c(0,0,3,0))
plot(density(possibleMeanDiffsH0),
     ylim=c(0,.04),
     xlim=range(possibleMeanDiffsH0),
     lwd=2,
     main="",#Mean Difference Test Statistic",
     xlab="Mean Differences Consistent with H0",cex.lab=0.75)
rug(possibleMeanDiffsH0)
rug(observedMeanTZ,lwd=3,ticksize = .51)
text(observedMeanTZ+8,.022,"Observed Test Statistic")

plot(density(possibleMeanRankDiffsH0),lwd=2,
     ylim=c(0,.45),
     xlim=c(-10,10), #range(possibleMeanDiffsH0),
     main="", #Mean Difference of Ranks Test Statistic",
     xlab="Mean Difference of Ranks Consistent with H0",cex.lab=0.75)
rug(possibleMeanRankDiffsH0)
rug(observedMeanRankTZ,lwd=3,ticksize = .9)
text(observedMeanRankTZ,.45,"Observed Test Statistic")

mtext(side=3,outer=TRUE,text=expression(paste("Distributions of Test Statistics Consistent with the Design and ",H0: y[i1]==y[i0])))
```

To formalize the comparison between observed and hypothesized, we can calculate the proportion of the hypothetical experiments that yield test statistics greater than the observed experiment.
In the left panel of the figure we see that a wide range of differences of means between treated and control groups are compatible with the treatment having no effects (with the overall range between `r min(possibleMeanDiffsH0)` and `r max(possibleMeanDiffsH0)`).
 The right panel shows that transforming the outcomes to ranks before taking the difference of means reduces the range of the test statistics --- after all the ranks themselves go from 1 to 10 rather than from 1 to 280.

### One-sided $p$-values

The one-sided $p$-values are `r pMeanTZ` for the simple mean difference and `r signif(pMeanRankTZ,2)` for the mean difference of the rank-transformed outcomes.
Each test statistic casts a different amount of doubt, or quantifies a different amount of surprise, about the same null hypothesis of no effects.
The outcome itself is so noisy that the mean difference of the rank-transformed outcomes does a better job of picking up the signal than the simple mean difference.
These data were generated with treatment effects built in, so the null hypothesis of no effects is false, but the information about the effects is noisy --- the sample size is small, and the distribution of the outcomes involves some strange outlying points and treatment effects themselves vary greatly.

### Two-sided $p$-values

Say we did not know in advance whether our experiment would show a negative effect or a positive effect.
Then we might make two hypothesis tests --- one calculating the one-sided upper $p$-value and the other calculating the one-sided lower $p$-value.
Now, if we did this we would be calculating two $p$-values and, if we made a standard practice of this, we would run the risk of misleading ourselves.
After all, recall from the [10 things about multiple comparisons](https://egap.org/resource/10-things-to-know-about-multiple-comparisons)
that even if there really is no effect, 100 independent and well operating tests of the null of no effects will yield no more than 5 $p$-values less than .05.
One easy solution to the challenge of summarizing extremity of a experiment in either direction rather than just focusing on greater-than or less-than is to calculate a two-sided $p$-value.
This, by the way, is the standard $p$-value produced by most canned software such as `lm()` and `t.test()` and `wilcox.test()` in R.
The basic idea is to calculate both $p$-values and then multiply the smaller $p$-value by 2.
(The idea here is that you are penalizing  yourself for making  two  tests -- see @rosenbaum2010design, Chap 2 and @cox1977role for  more on the idea of multiplying the smaller p-value by two.)

```{r twosidedp}
## Here I use <= and >= rather than < and > because of the discreteness of the
## randomization distribution with only 10 observations. See discussions of
## the "mid-p-value" 
p2SidedMeanTZ <-     2*min( mean( possibleMeanDiffsH0 >= observedMeanTZ ),
			   mean( possibleMeanDiffsH0 <= observedMeanTZ ))

p2SidedMeanRankTZ <- 2*min( mean( possibleMeanRankDiffsH0 >= observedMeanRankTZ ),
mean( possibleMeanRankDiffsH0 <= observedMeanRankTZ ))

```

In this case the two-sided $p$-values are `r p2SidedMeanTZ` and `r p2SidedMeanRankTZ` for the simple mean differences and means differences of ranks respectively.
We interpret them in terms of "extremity" --- we would only see an observed mean difference as far away from zero as the one manifest in our results roughly 18% of the time, for example.


**As a side note** The test of the sharp null shown here can be done without writing the code yourself.
The code that you'll see here (by clicking the code buttons) shows how  to use different R packages to  test hypotheses  using randomization-based inference.

```{r}
## using the coin package
library(coin)
set.seed(12345)
pMean2 <- pvalue(oneway_test(Y~factor(Z),data=dat,distribution=approximate(nresample=1000)))
dat$rankY <- rank(dat$Y)
pMeanRank2 <- pvalue(oneway_test(rankY~factor(Z),data=dat,distribution=approximate(nresample=1000)))
pMean2
pMeanRank2
## using a development version of the RItools package 
library(devtools)
dev_mode()
install_github("markmfredrickson/RItools@randomization-distribution")
library(RItools)
thedesignA <- simpleRandomSampler(total=N,z=dat$Z,b=rep(1,N))
pMean4 <- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanTZ ,
		 sampler = thedesignA)
pMeanRank4 <- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanrankTZ ,
		     sampler = thedesignA)
pMean4
pMeanRank4
dev_mode()

## using the ri2 package
library(ri2)
thedesign <- declare_ra(N=N)
pMean4 <- conduct_ri( Y ~ Z, declaration = thedesign, 
		     sharp_hypothesis = 0, data = dat, sims = 1000)
summary(pMean4)
pMeanRank4 <- conduct_ri( rankY ~ Z, declaration = thedesign, 
		     sharp_hypothesis = 0, data = dat, sims = 1000)
summary(pMeanRank4)
```


## An example: Testing the weak null of no average effects

The weak null hypothesis is a claim about aggregates, and is nearly always stated in terms of averages: $H_0: \bar{y}_{1} = \bar{y}_{0}$ The test statistic for this hypothesis nearly always is the difference of means (i.e. `meanTZ()` above.
The below code shows the use of least squares (`lm()` in R)
for the purpose of  calculating differences of means as a  test statistic for hypotheses about average effects.
Notice that the OLS-based $p$-values differ from those  calculated by `t.test()`  and `difference_of_means()`.
Recall that the OLS statistical inference is justified by the assumption of independent and identically distributed observations yet, in most experiments, the treatment itself changes the variation in the treatment group (thereby violating the identical-distribution/homoskedasticity assumption of OLS).
This is one of a few reasons why best practice in testing the  weak  null hypothesis of  no average treatment effects uses tools other than  those provided by simple canned OLS procedures.

```{r}
lm1 <- lm(Y~Z,data=dat)
lm1P <- summary(lm1)$coef["Z","Pr(>|t|)"]
ttestP1 <- t.test(Y~Z,data=dat)$p.value
library(estimatr)
ttestP2 <- difference_in_means(Y~Z,data=dat)
c(lmPvalue=lm1P,
ttestPvalue=ttestP1,
diffOfMeansPvalue=ttestP2$p.value)
```

This code produces  the same results without using least squares --- after all, we are just calculating differences of means and the variances of those means as they might vary across repeated experiments  in the same  pool of experimental units.

```{r byhand}
varEstATE <- function(Y,Z){
	var(Y[Z==1])/sum(Z) + var(Y[Z==0])/sum(1-Z)
}
seEstATE <- sqrt(varEstATE(dat$Y,dat$Z))
obsTStat <- observedMeanTZ/seEstATE
c(observedTestStat=observedMeanTZ,stderror=seEstATE,tstat=obsTStat,
  pval=2*min(pt(obsTStat,df=8,lower.tail = TRUE),
	     pt(obsTStat,df=8,lower.tail = FALSE))
  )
```

Notice that these tests all assume that the distribution of the test statistic across repeated experiments would be well characterized by a $t$-distribution.
The left-hand panel in the figure above shows the realized distribution of one way for the weak null to be true (i.e. if the sharp null is true): there are many ways for the weak null to be true --- some of which are compatible with large positive effects on some units and large negative effects on other units, others are compatible with other patterns of individual level effects.
In this particular small data set, engineered to have a very skewed outcome distribution, however, none of those patterns will produce a reference distribution that look like a Normal or $t$-curve if the mean difference is used as a test statistic.
We will return to this point later when we discuss the characteristics of a good test --- one of which is a controlled false positive rate.

# In simple hypothesis tests, we do not accept null hypotheses.

Sometimes people want to make a decision using the $p$-value.
Remember that a $p$-value uses a test statistic and the idea of repeating the experiment to quantifies information from the research design about a hypothesis.
It is the design, test statistic function and hypothesis which generates a probability distribution.
And it is the actual data, design, and test statistic function that creates a single observed value.

The $p$-value just tells us how extreme the observed result is from the perspective of the hypothesis.
Or, we can think of the $p$-value as encoding the inconsistency between our observed data and the hypothesis.
What if we want to make a decision? It turns out that **we can make decisions using a $p$-value if we are willing to accept a certain amount of error.** Say, for example, we see a one-tailed $p=.01$: this would mean that in only 1 in 100 hypothetical experiments representing the null hypothesis would we see a result as large or larger than our actual result.
We might be tempted to say, that our observed result is so strange that we want to act as if the null were false.
This would be ok---after all a $p$-value alone cannot control the behavior of an adult human---but the human has to know that in 1/100 cases where the null is true, we would still see this result on this same subject pool with this same experimental design.
That is, if we used a small $p$-value to reject the null, or acted as if the null were false we could still be making an error.
These incorrect rejections are sometimes called false positive errors because the null hypothesis is so often zero and the desired effect (say, in medical trials) is so often coded as positive.

Say we were happy to make 1 false positive error or false rejection in every 20 experiments.
In that case, we should also be happy to reject a null hypothesis if we saw a $p \le 1/20$ or  $p \le .05$.
 And we would call $p$-values smaller than .05 signals of inconsistency with the null hypothesis and thus should only lead us to err in 5% of experiments like the one that we are analyzing.

## What does it mean to reject a null hypothesis?

Notice that a $p=.01$ only reflects extremity of the observed data compared with the hypothesis --- it means that the observed test statistic looks extreme when considered from the perspective of the distribution of tests statistics that are generated from the null hypothesis and research design.
So, we think of $p=.01$ (and other small $p$-values) as casting doubt on whether the specific hypothesis is a good model of the observed data.
Often the only model of potential outcomes that is tested is the model of no effects, so a small $p$-value should make us doubt the model of no effects.
The makers of canned regression software tend to print out a $p$-value that refers to this hypothesis  automatically,  so that it is difficult to not see the results of this test even if you just want to describe differences of means in the data but you are using least squares as your difference of means calculator.

## What does it mean to **not reject** a null hypothesis?

Notice that a $p=.50$ only reflects extremity of the observed data compared with the hypothesis---but the observed data, in this case, do not look extreme but common from the perspective of the null hypothesis.
So, $p=.5$ (and other large p-values) do not encourage us to doubt the model of the null hypothesis.
It does not encourage us to accept that model---it is only a model after all.
We do not know reasonable the model was a priori, for example.
So, a single large $p$-value is some argument in favor of the null, but not a very clear argument.


# Once you are using $p$-values to reject a hypothesis, you will make errors

A good test rejects true hypotheses rarely (i.e. has a controlled false positive error rate) and easily detects signal from noise (i.e. has good statistical power, a rarely makes the error of missing the signal in the noise).


## How to learn about errors of missing the signal in the noise? 

The [10 Things you need to know about statistical power](https://egap.org/resource/10-things-to-know-about-statistical-power) guide explains how we want hypotheses to reject false nulls (i.e. detect signal from noise).
When we think about the power of statistical tests, we need to consider the **alternative hypothesis**.
However, as we have shown above, we can test **null hypotheses** without the idea of rejecting or accepting them although then the "power" of a test is harder to define and work with.

## How to learn about false positive errors?

The easiest way to learn about false positive errors is by simulation.
First, we create the situation where the null is true and known, and then we test that null under the many ways that it is possible for that null to be true.
For example, in the example experiment used here, we have 5 units assigned to treatment out of 10.
This means that there are  $\binom{10}{5}=252$ different ways to assign treatment --- and  252 ways for the experiment to have had no effects on the individuals.

We demonstrate here setting the sharp or strict null hypothesis to be zero, but one could also assess the false positive rate different hypotheses.
We compare error rates for a few of the approaches used so far, including the test of the weak null of no effects.
The following plot shows the proportion of $p$-values less than any given level of significance (i.e. rejection threshold) for each of four tests.
That is, this is a plot of false positive rates for any given significance threshold.
A test that has a controlled or known false positive rate would have symbols on or below the line across the whole x-axis or range of the plot.
As we can see here, the two tests using permutations of treatment to assess the sharp null of no effects have this feature.
The tests of the weak null using the mean difference test statistic and appealing to the large sample theory to justify the use of a $t$-distribution do not have a controlled false positive rate: the proportion of $p$-values below any given rejection threshold can be too high or too low.

```{r}
collectPValues <- function(y,z){
	## Make Y and Z have no relationship by re-randomizing Z
	newz <- repeatExperiment(z)
        thelm <- lm(y~newz,data=dat)
	ttestP2 <- difference_in_means(y~newz,data=dat)
	owP <- pvalue(oneway_test(y~factor(newz),distribution=exact()))
	ranky <- rank(y)
	owRankP <- pvalue(oneway_test(ranky~factor(newz),distribution=exact()))
	return(c(lmp=summary(thelm)$coef["newz","Pr(>|t|)"]
,
		 neyp=ttestP2$p.value[[1]],
		 rtp=owP,
		 rtpRank=owRankP))
}
```

```{r fprdsim, cache=TRUE, results='hide'}
set.seed(12345)
pDist <- replicate(5000,collectPValues(y=dat$Y,z=dat$Z))
```

```{r}
par(mfrow=c(1,1),mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))
plot(c(0,1),c(0,1),type="n",
     xlab="p-value=p",ylab="Proportion p-values < p")
for(i in 1:nrow(pDist)){
	lines(ecdf(pDist[i,]),pch=i,col=i)
}
abline(0,1,col="gray")
legend("topleft",legend=c("OLS","Neyman","Rand Inf Mean Diff","Rand Inf Mean \n Diff Ranks"),
			  pch=1:5,col=1:5,lty=1,bty="n")
```

In this particular case, at the threshold of $\alpha=.05$, all of the tests except for the rank based test report less than a 5% false positive rate -- this is good, it should be 5% of less.
However, this is no  guarantee  of good performance by the large-sample based tests in other small experiments, or experiments with highly skewed  outcomes,  etc...
When in doubt it is  easy to assess the false positive rate  of a test by using the code in this guide to make your own simulation.

```{r fpr05}
apply(pDist,1,function(x){ mean(x<.05)})
```

# What else to know about hypothesis tests.

Here we list a few other important but advanced topics connected to hypothesis testing:

 - Even if a given testing procedure controls the false positive rate for   a single test, it may not control the rate for a group of multiple tests.
   See [10 Things you need to know about multiple   comparisons](https://egap.org/resource/10-things-to-know-about-multiple-comparisons)
   for a guide to the approaches to controlling such rejection-rates in   multiple tests.
 - A $100\alpha$\% confidence interval can be defined as the range of   hypotheses where all of the $p$-values are greater than or equal to   $\alpha$.
This is called inverting the hypothesis test.
   (@rosenbaum2010design).
That is, a **confidence interval is a collection of   hypothesis tests**.
This  means that criticisms of $p$-values are also   criticisms of confidence intervals.
 - A point estimate based on hypothesis testing is called a Hodges-Lehmann   point estimate.
(@rosenbaum1993hlp,@hodges1963elb)
 - A set of hypothesis tests can be combined into one single hypothesis test.   For example, you can test the hypothesis of an effect of  size 1 on   outcome 1, an effect  of size 0  on outcome 2 and an effect of -10 on   outcome 3.
(@hansen:bowers:2008,@caughey2017nonparametric)
 - In equivalence testing, one can hypothesize that two test-statistics are   equivalent (i.e.the treatment group is the same as the control group)   rather than only about one test-statistic (the difference between the two   groups is zero) {@hartman2018equivalence}
 - Since a hypothesis test is a model of potential outcomes, one can use   hypothesis testing to learn about complex models, such as models of   spillover and propagation of treatment effects across networks
   (@bowers2013reasoning, @bowers2016research, @bowers2018models)



# References


