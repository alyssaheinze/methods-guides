---
output:
  html_document:
    toc: true
    theme: journal
---

<!-- title: "10 choses à savoir sur le contrôle des covariables pour les essais randomisés contrôlés (ECRs)” -->
<!-- author: "Auteur du guide des méthodes: Lindsay Dolan" -->

Résumé
==
Ce guide[^1] vous aidera à déterminer quand il est judicieux d'essayer de "contrôler d'autres choses" lors de l'estimation de l'effet du traitement à l'aide de données expérimentales.
Nous nous concentrons sur les grandes idées et fournissons des exemples dans R.

[^1] : Auteur d'origine : Lindsay Dolan. Révisions : Don Green et Winston Lin, 1er novembre 2016.
Le guide est un document vivant et susceptible d'être mis à jour par les membres de l'EGAP à tout moment ; les contributeurs répertoriés ne sont pas responsables des modifications ultérieures.
Merci à Macartan Humphreys et Diana Mutz pour les discussions utiles.

1 Qu'est-ce que l'ajustement de covariable ?
==
Les "covariables" sont les caractéristiques de base de vos sujets expérimentaux.
Lorsque vous exécutez une expérience, vous êtes principalement intéressé par la collecte de données des variables de résultat que votre intervention peut affecter,
i.e. les décisions de dépenses, les attitudes envers la démocratie ou les contributions à un bien public dans une expérience de laboratoire.
Mais c'est aussi une bonne idée de collecter des données sur les caractéristiques de base des sujets avant l'assignation du traitement, i.e. le sexe, le niveau d'éducation ou le groupe ethnique.
Si vous faites cela, vous pouvez explorer comment l'effet du traitement varie en fonction de ces caractéristiques (voir [10 choses à savoir sur les effets de traitement hétérogènes](https://egap.org/resource/10-things-to-know-about-heterogeneous-treatment-effects)).
Mais cela vous permet également d'effectuer un ajustement de covariable.

L'ajustement de covariable est un autre nom pour contrôler les variables de base lors de l'estimation de l'effet du traitement.
Souvent, cela est fait pour améliorer la précision.
Les résultats des sujets sont susceptibles d'avoir une certaine corrélation avec des variables qui peuvent être mesurées avant l'assignation aléatoire.
La prise en compte de variables telles que le sexe vous permettra de mettre de côté la variation des résultats qui est prédite par ces variables de base, afin que vous puissiez isoler l'effet du traitement sur les résultats avec plus de précision et de puissance.

L'ajustement des covariables peut être un moyen moins coûteux d'améliorer la précision plutôt que d'augmenter le nombre de sujets dans l'expérience.
C'est en partie pour cette raison que les chercheurs recueillent souvent de nombreuses données sur les covariables avant l'assignation aléatoire.
Les pré-tests (mesures analogues à la variable de résultat mais limitées aux périodes précédant l'assignation aléatoire) peuvent être particulièrement utiles pour prédire les résultats, et les enquêtes initiales peuvent interroger les sujets sur d'autres caractéristiques de base.

2 Contrôle des covariables au stade du design (découpage par bloc)
==
La meilleure façon de contrôler les covariables est d'utiliser la randomisation par bloc pour le faire au stade du design avant même de commencer votre expérience.
La randomisation par bloc vous permet de créer des groupes de traitement et de contrôle équilibrés pour certaines covariables.
Par exemple, vous pourriez vous attendre à ce que le sexe et le revenu aident à prédire la variable de résultat.
La randomisation par bloc peut garantir que les groupes de traitement et de contrôle ont des proportions égales de populations de femmes/à revenu élevé, de femmes/à faible revenu, d'homme/à revenu élevé et d'homme/à faible revenu.
Lorsque les variables utilisées pour les blocs aident à prédire les résultats, le découpage par bloc améliore la précision en empêchant les corrélations aléatoires entre l'assignation du traitement et les covariables de base.

Pour plus d'informations sur le découpage par bloc et comment l'implémenter dans R, voir [10 choses à savoir sur la randomisation](https://egap.org/resource/10-things-to-know-about-randomization).
Les gains de précision du découpage par bloc (par rapport à l'ajustement de covariable sans bloc) ont tendance à être plus importants lorsque la taille des échantillons est petite.^[Miratrix, Luke W., Jasjeet S. Sekhon et Bin Yu (2013). "Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments." _Journal of the Royal Statistical Society, Series B_ 75: 369--396.]

Lorsque le découpage par bloc est effectué pour améliorer la précision, l'erreur standard estimée doit tenir compte du découpage.
(Sinon, l'erreur standard aura tendance à être conservatrice car elle ne vous donnera pas le crédit de l'amélioration de la précision obtenue par le découpage.)
Une méthode simple et couramment utilisée consiste à régresser le résultat sur la variable muette d'assignation du traitement ainsi que sur les variables muettes de bloc.
Lorsque la probabilité d'assignation au traitement est constante d'un bloc à l'autre, l'inclusion des variables muettes de bloc dans la régression ne modifie pas l'effet du traitement estimé, mais tend à donner une estimation plus précise de l'erreur standard.^[Voir, e.g., pages 217--219 de Miriam Bruhn et David McKenzie (2009), "In Pursuit of Balance: Randomization in Practice in Development Field Experiments," _American Economic Journal: Applied Economics_ 1 (4): 200--232.]

Si la probabilité d'assignation au traitement varie d'un bloc à l'autre, vous devez alors contrôler ces probabilités inégales afin d'obtenir des estimations non biaisées de l'effet moyen du traitement. [10 choses à savoir sur la randomisation](https://egap.org/resource/10-things-to-know-about-randomization) l'aborde de manière pratique.

3 Comment le faire dans une régression
==
Sometimes you do not have the opportunity to implement a blocked experimental design (for example, if you join a project after random assignment occurs) or you would prefer to simplify your randomization scheme to reduce opportunities for administrative error.
You can still adjust for covariates on the back end by using multiple regression.
Remember that in a bivariate regression---when you regress your outcome on just your treatment indicator---the coefficient on treatment is just a difference-in-means.
 This simple method gives an unbiased estimate of the average treatment effect (ATE).
When we add baseline covariates that are correlated with outcomes to the model, the coefficient on treatment is an approximately unbiased estimate of the ATE that tends to be more precise than bivariate regression.

Parfois, vous n'avez pas la possibilité de mettre en œuvre un design expérimental par bloc (i.e., si vous rejoignez un projet après l'assignation aléatoire) ou vous préférez simplifier votre schéma de randomisation pour réduire les risques d'erreur administrative.
Vous pouvez toujours ajuster les covariables de base en utilisant la régression multiple.
N'oubliez pas que dans une régression bivariée --- lorsque vous régressez votre résultat uniquement sur votre indicateur de traitement --- le coefficient de traitement n'est qu'une différence des moyennes.
Cette méthode simple donne une estimation non biaisée de l'effet moyen du traitement (ATE).
Lorsque nous ajoutons au modèle des covariables de base corrélées aux résultats, le coefficient de traitement est une estimation approximativement non biaisée de l'ATE qui a tendance à être plus précise que la régression bivariée.

Pour ajuster les covariables par régression multiple, utilisez le modèle :

$$Y_i = \alpha + \beta Z_i + \gamma X_i + \epsilon_i$$

où $Y_i$ est la variable de résultat, $Z_i$ est l'indicateur de traitement et $X_i$ est un vecteur d'une ou plusieurs covariables.
Le reste $\epsilon_i$ est votre terme de perturbation --- le bruit inexpliqué restant.

Lorsque les groupes de traitement et de contrôle sont de taille inégale, les gains de précision de l'ajustement des covariables peuvent être plus importants si vous incluez les interactions entre le traitement et les covariables (voir [ce blog](https://web.archive.org/web/20151024055802 /http://blogs.worldbank.org/impactevaluations/node/847) pour plus d'information).
Pour faciliter l'interprétation, recentrez les covariables pour avoir une moyenne nulle :

$$Y_i = \alpha + \beta Z_i + \gamma W_i + \delta Z_i*W_i + \epsilon_i$$

où $W_i = X_i - \overline{X}$ et $\overline{X}$ est la valeur moyenne de $X_i$ pour l'ensemble de l'échantillon.

Si les sujets reçoivent différentes probabilités d'assignation au traitement en fonction de leurs covariables, alors notre méthode d'estimation doit en tenir compte (encore une fois, voir [10 choses à savoir sur la randomisation](https://egap.org/resource/10-things-to-know-about-randomization) pour plus de détails).

4 Pourquoi le faire
==

Il n'est pas absolument nécessaire de contrôler les covariables lors de l'estimation de l'effet moyen du traitement dans un ECR qui attribue à chaque sujet la même probabilité de recevoir le traitement.
La différence des moyennes traitement-contrôle non ajustée pour les résultats est un estimateur sans biais de l'ATE.
Cependant, l'ajustement des covariables a tendance à améliorer la précision si les covariables sont de bons prédicteurs du résultat.^[Un bref examen du biais et de la précision : imaginez répéter l'expérience plusieurs fois (sans changer l'échantillon expérimental et les conditions, mais en refaisant l'assignation aléatoire du traitement chaque fois).
Un estimateur sans biais peut surestimer ou sous-estimer l'ATE sur une répétition donnée, mais son espérance (la moyenne sur toutes les répétitions possibles) sera égale à l'ATE réel.
Nous préférons généralement des estimateurs sans biais ou approximativement sans biais, mais nous valorisons également la précision (qui est formellement définie comme l'inverse de la variance).
Imaginez que vous lancez une fléchette sur un jeu de fléchettes.
Si vous frappez le centre de la cible en moyenne mais que vos tirs sont souvent loin du but, vous disposez d'un estimateur impartial mais imprécis.
Si vous frappez près du centre à chaque fois, votre estimateur est plus précis.
Un chercheur peut choisir d'accepter un petit biais en échange d'une grande amélioration de la précision.
Un critère possible pour évaluer les estimateurs est l'[erreur quadratique moyenne](https://fr.wikipedia.org/wiki/Erreur_quadratique_moyenne), qui est égale à la variance plus le carré du biais.
Voir, par exemple, Sharon Lohr (2010), _Sampling: Design and Analysis_, 2e éd., pp. 31--32.]

Dans les grands échantillons, l'assignation aléatoire a tendance à produire des groupes de traitement et de contrôle avec des caractéristiques de base similaires.
Pourtant, par "chance du tirage au sort", un groupe peut être légèrement plus éduqué, ou un groupe peut avoir des taux de vote légèrement plus élevés lors des élections précédentes, ou un groupe peut être légèrement plus âgé en moyenne.
Pour cette raison, l'ATE estimé est soumis à une "variabilité d'échantillonnage", ce qui signifie que vous obtiendrez des estimations de l'ATE qui ont été produites par une méthode impartiale, mais qui ont manqué la cible.[^3] Une variabilité d'échantillonnage élevée contribue au bruit (imprécision), pas au biais.

[^3] : la "variabilité d'échantillonnage" fait référence à la dispersion des estimations qui seront produites simplement en raison des différentes assignations aléatoires qui auraient pu être tirées.
Lorsque le tirage au sort pour l'assignation aléatoire produit un groupe de traitement avec plus d'As et un groupe témoin avec plus de Bs, il est plus difficile de séparer les caractéristiques de base (A et B) de l'assignation de traitement en tant que prédicteur des résultats observés.

Le contrôle des covariables a tendance à améliorer la précision si les covariables sont prédictives des résultats potentiels.
Jetez un œil à l'exemple suivant, qui est vaguement basé sur l'expérience de Giné et Mansuri sur le comportement électoral des femmes au Pakistan.^[Giné, Xavier et Ghazala Mansuri (2012).
["Together We Will: Experimental Evidence on Female Voting Behavior in Pakistan."](http://siteresources.worldbank.org/DEC/Resources/gine_mansuri_voting_ReStat.pdf)]
Dans cette expérience, les auteurs ont randomisé une campagne d'information auprès de femmes en Pakistan pour étudier ses effets sur leur comportement de participation, l'indépendance de leur choix de candidat et leurs connaissances politiques.
Ils ont réalisé une enquête de base qui leur a fourni plusieurs covariables.

Le code suivant imite cette expérience en créant de fausses données pour quatre des covariables qu'ils collectent : la posséssion d'une carte d'identité, la scolarité, l'âge et l'accès à la télévision.
Cela crée également deux [résultats potentiels](https://egap.org/resource/10-things-to-know-about-causal-inference) (les résultats qui se produiraient si elle était affectée au traitement ou non) pour mesurer à quel point la femme a voté indépendamment des opinions des hommes dans sa famille.
Les résultats potentiels sont corrélés avec les quatre covariables, et l'effet de traitement "vrai" intégré sur la mesure d'indépendance est ici de 1.
Pour déterminer si notre estimateur est biaisé ou non, nous simulons 10 000 répétitions de notre expérience.
À chaque répétition, nous assignons le traitement au hasard, puis régressons le résultat observé $Y$ sur l'indicateur de traitement $Z$, avec et sans contrôle des covariables.
Ainsi, nous simulons deux méthodes (non corrigées et corrigées par les covariables) pour estimer l'ATE.
Pour estimer le biais de chaque méthode, nous prenons la différence des moyennes pour 10 000 estimations simulées et le "vrai" effet de traitement.

```{r , error=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
rm(list=ls())

set.seed(20140714)
N = 2000
N.treated = 1000
Replications = 10000

true.treatment.effect = 1

# Créer des covariables de pré-traitement
owns.id.card = rbinom(n = N, size = 1, prob = .18)
has.formal.schooling = rbinom(n = N, size = 1, prob = .6)
age = round(rnorm(n = N, mean = 37, sd = 16))
age[age<18] = 18
age[age>65] = 65
TV.access = rbinom(n = N, size = 1, prob = .7)
epsilon = rnorm(n = N, mean = 0, sd = 2)

# Créer des résultats potentiels corrélés aux covariables pré-traitement
Y0 = round(owns.id.card + 2*has.formal.schooling + 3*TV.access + log(age) + epsilon)
Y1 = Y0 + true.treatment.effect

# Répéter l'assignation du traitement
Z.mat = replicate(Replications, ifelse(1:N %in% sample(1:N, N.treated), 1, 0))

# Générer des résultats observés
Y.mat = Y1 * Z.mat + Y0 * (1 - Z.mat)

diff.in.means = function(Y, Z) {
  coef(lm(Y ~ Z))[2]
}

ols.adjust = function(Y, Z) {
  coef(lm(Y ~ Z + owns.id.card + has.formal.schooling + age + TV.access))[2]
}

unadjusted.estimates = rep(NA, Replications)
adjusted.estimates   = rep(NA, Replications)

for (i in 1:Replications) {
  unadjusted.estimates[i]  =  diff.in.means(Y.mat[,i], Z.mat[,i])
  adjusted.estimates[i]    =  ols.adjust(Y.mat[,i], Z.mat[,i])
}

# Variabilité estimée (écart type) de chaque estimateur
sd.of.unadj = sd(unadjusted.estimates)
sd.of.unadj
sd.of.adj   = sd(adjusted.estimates)
sd.of.adj

# Biais estimé de chaque estimateur
mean(unadjusted.estimates) - true.treatment.effect
mean(adjusted.estimates) - true.treatment.effect

# Marge d'erreur (avec un niveau de confiance de 95 %) pour chaque biais estimé
1.96 * sd.of.unadj / sqrt(Replications)
1.96 * sd.of.adj   / sqrt(Replications)
```

Les deux méthodes --- avec et sans covariables --- donnent le véritable effet de traitement de 1 en moyenne.
Lorsque nous avons exécuté la régression sans covariables, notre ATE estimé était en moyenne de 1,0008 sur les 10 000 répétitions, et avec les covariables, il était en moyenne de 1,0003.
Notez que l'estimation ajustée par régression est essentiellement sans biais même si notre modèle de régression est mal spécifié --- nous contrôlons l'âge de manière linéaire lorsque le véritable processus de génération de données implique le log de l'âge.^[Le biais estimé est de 0,0003 avec une marge d'erreur (au niveau de confiance de 95 %) de 0,0018.]

Les vrais gains viennent de la précision de nos estimations.
L'erreur standard (l'écart type de la distribution d'échantillonnage) de notre ATE estimé lorsque nous ignorons les covariables est de 0,121.
Lorsque nous incluons des covariables dans le modèle, notre estimation devient un peu plus stricte : l'erreur standard est de 0,093.
Parce que nos covariables étaient prédictives de nos résultats, les inclure dans la régression a expliqué une partie du bruit présent dans nos données afin que nous puissions resserrer notre estimation de l'ATE.

5 Quand cela sera-t-il utile ?
==
Quand l'ajustement pour les covariables est-il le plus susceptible d'améliorer la précision ?

L'ajustement des covariables sera plus utile lorsque vos covariables sont fortement prédictives (ou "pronostiques") de vos résultats.
L'ajustement des covariables vous permet essentiellement d'utiliser des informations sur les relations entre les caractéristiques de base et votre résultat afin que vous puissiez mieux identifier la relation entre le traitement et le résultat.
Mais si les caractéristiques de base ne sont que faiblement corrélées avec le résultat, l'ajustement des covariables ne vous sera d'aucune utilité.
Les covariables que vous voudrez ajuster sont celles qui sont fortement corrélées aux résultats.

Le graphique suivant montre la relation entre le pronostic de votre covariable et les gains que vous obtenez en l'ajustant.
En abscisse se trouve la taille de l'échantillon, et en ordonnée se trouve la [racine de l'erreur quadratique moyenne](https://fr.wikipedia.org/wiki/Racine_de_l%27erreur_quadratique_moyenne) (RMSE), entre l'estimateur et le vrai ATE.
Nous voulons que notre RMSE soit faible, et l'ajustement des covariables devrait nous aider à le réduire.

```{r, message=FALSE, warning=FALSE, error=FALSE, eval=FALSE}
rm(list=ls())
library(MASS)  # pour mvrnorm()
set.seed(1234567)
num.reps = 10000

# Le véritable effet du traitement est de 0 pour chaque unité

adj.est = function(n, cov.matrix, treated) {
    Y.and.X  =  mvrnorm(n, mu = c(0, 0), Sigma = cov.matrix)
    Y   =  Y.and.X[, 1]  
    X   =  Y.and.X[, 2]
    coef(lm(Y ~ treated + X))[2]
}

unadj.est = function(n, treated) {
    Y = rnorm(n)
    coef(lm(Y ~ treated))[2]
}

rmse = function(half.n, rho = 0, control = TRUE) {
    treated  =  rep(c(0, 1), half.n)
    n = 2 * half.n

    if (control) {
        cov.matrix  =  matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)
        return( sqrt(mean(replicate(num.reps, adj.est(n, cov.matrix, treated)) ^ 2)) )
    }
    else {
        return( sqrt(mean(replicate(num.reps, unadj.est(n, treated)) ^ 2)) )
    }
}

half.n = c(5, 7, 11, 19, 35, 67, 131)
n = 2 * half.n 
E  = sapply(half.n, rmse, control = FALSE)
E0 = sapply(half.n, rmse, rho = 0)
E1 = sapply(half.n, rmse, rho = 0.5)
E2 = sapply(half.n, rmse, rho = 0.9)

plot(n, E, type = "l", ylab = "RMSE", xlim = c(min(n),max(n)), ylim = c(0,.75))
lines(n, E0, col = "yellow")
lines(n, E1, col = "orange")
lines(n, E2, col = "red")
legend(x = 'topright',
       c("No controls",
         expression(paste(rho, "=0")), expression(paste(rho, "=0.5")),
         expression(paste(rho, "=0.9"))),
         col=c("black", "yellow","orange", "red"), lty = 1, lwd=2)
```

![](rmse.png)

La ligne noire montre le RMSE lorsque nous n'ajustons pas pour une covariable.
La ligne rouge montre le RMSE lorsque nous ajustons pour une covariable fortement prédictive (la corrélation entre la covariable et le résultat est de 0,9).
Vous pouvez voir que la ligne rouge est toujours en dessous de la ligne noire, c'est-à-dire que le RMSE est plus faible lorsque vous ajustez pour une covariable prédictive.
La ligne orange représente le RMSE lorsque nous ajustons pour une covariable de pronostic modéré (la corrélation entre la covariable et le résultat est de 0,5).
Nous obtenons toujours des gains de précision par rapport à la ligne noire, mais pas autant qu'avec la ligne rouge.
Enfin, la ligne jaune montre ce qui se passe si vous contrôlez une covariable qui n'est pas du tout prédictive du résultat.
La ligne jaune est presque identique à la ligne noire.
Vous n'avez obtenu aucune amélioration de la précision en contrôlant une covariable non prédictive ; en fait, vous avez payé une légère pénalité parce que vous avez perdu un degré de liberté, ce qui est particulièrement coûteux lorsque la taille de l'échantillon est petite.
Cet exercice démontre que vous obtiendrez le plus de gains de précision en contrôlant les covariables qui prédisent fortement les résultats.

Comment savoir quelles covariables sont susceptibles d'être prédictives avant de lancer votre expérience ? Des expériences antérieures ou même des études d'observation peuvent offrir des indications sur les caractéristiques de base qui prédisent le mieux les résultats.

6 Contrôle des covariables prédictives, qu'elles présentent ou non des déséquilibres
==

Les covariables doivent généralement être choisies en fonction de leur capacité attendue à aider à prédire les résultats, qu'elles présentent ou non des "déséquilibres" (i.e., qu'il existe ou non des différences notables entre le groupe de traitement et le groupe de contrôle dans les valeurs moyennes ou d'autres aspects des distributions de covariables).
Il y a deux raisons à cette recommandation :

1. L'inférence fréquentiste (erreur standard, intervalles de confiance, p-valeurs, etc.) suppose que l'analyse suit une stratégie prédéfinie.
Choisir des covariables sur la base des déséquilibres observés rend plus difficile l'obtention d'inférences qui reflètent votre stratégie réelle.
Par exemple, supposons que vous choisissiez de ne pas contrôler le genre parce que les groupes de traitement et de contrôle ont une composition similaire pour le genre, mais vous _auriez_ contrôlé le genre s'il y avait eu un déséquilibre notable.
Les méthodes typiques d'estimation de l'erreur standard supposeront à tort que vous ne contrôlerez jamais le sexe, quel que soit le déséquilibre que vous constatez.

2. L'ajustement pour une covariable fortement prédictive a tendance à améliorer la précision, comme nous l'avons expliqué ci-dessus.
Pour recevoir le crédit dû pour cette amélioration de la précision, vous devez ajuster la covariable même s'il n'y a pas de déséquilibre.
Par exemple, supposons que le sexe soit fortement corrélé avec votre résultat, mais il arrive que les groupes de traitement et de contrôle aient exactement la même composition de genre.
Dans ce cas, l'estimation non ajustée de l'ATE sera exactement la même que l'estimation ajustée à partir d'une régression du résultat sur le traitement et le sexe, mais leurs erreurs standard seront différentes.
L'erreur standard de l'estimation non ajustée a tendance à être plus grande car elle suppose que même si les groupes de traitement et de contrôle avaient des compositions de genre très différentes, vous utiliseriez toujours la différence traitement-contrôle non ajustée dans les résultats moyens (qui serait probablement loin du vrai ATE dans ce cas).
Si vous spécifiez à l'avance que vous ajusterez pour le sexe, quel que soit l'ampleur du déséquilibre que vous constatez, vous aurez tendance à obtenir une erreur standard plus petite, un intervalle de confiance plus serré et un test de signification plus puissant.

En supposant que l'assignation aléatoire a été mise en œuvre correctement, l'examen des déséquilibres devrait-il jouer un rôle _quelconque_ dans le choix des covariables à corriger ?
Voici un échantillon de points de vue :

- Mutz, Pemantle et Pham (2016) soutiennent que, à moins qu'il n'y ait une attrition différentielle, la pratique consistant à sélectionner des covariables sur la base des déséquilibres observés est "non seulement inutile" mais "même pas utile ... et peut en fait être préjudiciable", car cela invalide les intervalles de confiance, détériore la précision (par rapport à l'ajustement prédéfini pour les covariables prédictives) et ouvre la porte à tout.^[Diana C. Mutz, Robin Pemantle et Philip Pham (2016), ["Model Choice in Experimental Design: Messy Analyses of Clean Data."](https://www.math.upenn.edu/~pemantle/papers/Preprints/balance.pdf)]

- Permutt (1990), en utilisant la théorie et des simulations pour étudier des scénarios spécifiques, constate que lorsqu'un test d'équilibre est utilisé pour décider s'il faut ajuster pour une covariable, le test de signification pour l'effet du traitement est prudent (c'est-à-dire qu'il a un vrai type I probabilité d'erreur inférieure à son niveau nominal).
Il écrit : "Une plus grande puissance peut être obtenue en ajustant toujours pour une covariable qui est fortement corrélée avec la réponse quelle que soit sa distribution entre les groupes." Cependant, il n'exclut pas complètement de considérer les déséquilibres observés : « Choisir des covariables sur la base de la différence entre les moyennes des groupes de traitement et de contrôle n'est pas irrationnel.
Après tout, certaines erreurs de type I peuvent être plus graves que d'autres.
Signaler une différence significative dans les résultats qui peut être expliquée comme l'effet d'une covariable peut être une erreur plus embarrassante que de signaler une qui disparaît lors de la réplication mais sans explication simple.
Des considérations similaires peuvent s'appliquer aux erreurs de type II.
Un résultat positif qui dépend de l'ajustement pour une covariable peut de toute façon être considéré comme moins convaincant qu'un test positif à deux échantillons, de sorte que l'erreur de ne pas tirer une conclusion aussi positive peut être moins grave.
Ces justifications, cependant, viennent de l'extérieur de la théorie formelle des tests d'hypothèses."^[Thomas Permutt (1990), "Testing for Imbalance of Covariates in Controlled Experiments," _Statistics in Medicine_ 9: 1455--1462.]

- Permutt (1990), using theory and simulations to study specific scenarios, finds that when a balance test is used to decide whether to adjust for a covariate, the significance test for the treatment effect is conservative (i.e., it has a true Type I error probability below its nominal level).
He writes, "Greater power can be achieved by always adjusting for a covariate that is highly correlated with the response regardless of its distribution between groups." However, he doesn't completely rule out considering observed imbalances: "Choosing covariates on the basis of the difference between the means in the treatment and control groups is not irrational.
After all, some type I errors may be more serious than others.
Reporting a significant difference in outcome which can be explained away as the effect of a covariate may be a more embarrassing error than reporting one that happens to go away on replication but without an easy explanation.
Similar considerations may apply to type II errors.
A positive result that depends on adjustment for a covariate may be seen as less convincing than a positive two-sample test anyway, so that the error of failing to draw such a positive conclusion may be less serious.
These justifications, however, come from outside the formal theory of testing hypotheses."^[Thomas Permutt (1990), "Testing for Imbalance of Covariates in Controlled Experiments," _Statistics in Medicine_ 9: 1455--1462.]

- Altman (2005) writes, "It seems far preferable to choose which variables to adjust for without regard to the actual data set to hand." He recommends controlling for highly prognostic covariates, as well as any that were used in blocking.
However, he also discusses a dilemma: "In practice, imbalance may arise when the possible need for adjustment has not been anticipated.
What should the researchers do? They might choose to ignore the imbalance; as noted, this would be entirely proper.
The difficulty then is one of credibility.
Readers of their paper (including reviewers and editors) may question whether the observed finding has been influenced by the unequal distribution of one or more baseline covariates.
It is still possible, and arguably advisable, to carry out an adjusted analysis, but now with the explicit acknowledgment that this is an exploratory rather than definitive analysis, and that the unadjusted analysis should be taken as the primary one.
Obviously, if the simple and adjusted analyses yield substantially the same result, then there is no difficulty of interpretation.
This will usually be the case.
However, if the results of the two analyses differ, then there is a real problem.
The existence of such a discrepancy must cast some doubt on the veracity of the overall (unadjusted) result.
The situation is similar to the difficulties of interpretation that arise with unplanned subgroup comparisons.
One suggestion in such circumstances is to try to mimic what would have been done if the problem had been anticipated, namely to adjust not for variables that are observed to be unbalanced, but for all variables that would have been identified in advance as prognostic.
An independent source could be used to identify such variables.
Alternatively, the trial data could be used to determine which variables are prognostic.
This strategy too could be prespecified in the study protocol.
Because this analysis would be performed conditionally on the observed imbalance, it does not remove bias and thus cannot be considered fully satisfactory."^[Douglas G. Altman (2005), ["Covariate Imbalance, Adjustment for,"](http://doi.org/10.1002/0470011815.b2a01015) in _Encyclopedia of Biostatistics_.]

- Tukey (1991) notes that observed imbalances may justify adjustment as a robustness check: Although "most statisticians" would accept an analysis of a randomized clinical trial that doesn't adjust for covariates, "Some clinicians, and some statisticians it would seem, would like to be more sceptical, (perhaps as a supplemental analysis) asking for an analysis that takes account of observed imbalances in these recorded covariates.
Feeling more secure about the results of such an analysis is indeed appropriate, since the degree of protection against either the consequences of inadequate randomization or the (random) occurrence of an unusual randomization is considerably increased by adjustment.
_Greater security, rather than increased precision ... will often be the basic reason for covariance adjustment in a randomized trial._ ...
The main purpose of allowing [adjusting] for covariates in a _randomized_ trial is defensive: to make it clear that analysis has met its scientific obligations."^[John W. Tukey (1991), "Use of Many Covariates in Clinical Trials," _International Statistical Review_ 59: 123--137. Italics in the original.]

- Some statisticians argue that our inferences should be conditional on a measure of covariate imbalance---in other words, when assessing the bias, variance, and mean squared error of a point estimate or the coverage probability of a confidence interval, instead of considering all possible randomizations, it may be more relevant to consider only those randomizations that would yield a covariate imbalance similar to the one we observe.
From this perspective, observed imbalances may be relevant to the choice of estimator.^[See, e.g.: D. R. Cox and N. Reid (2000), _The Theory of the Design of Experiments_, pp. 29--32; D. Holt and T. M. F. Smith (1979), "Post Stratification," _Journal of the Royal Statistical Society, Series A (General)_ 142: 33--46; Richard M. Royall (1976), "Current Advances in Sampling Theory: Implications for Human Observational Studies," _American Journal of Epidemiology_ 104: 463--474.
For an introduction to philosophical disagreements about statistical inference, see Bradley Efron (1978), ["Controversies in the Foundations of Statistics,"](http://www.maa.org/programs/maa-awards/writing-awards/controversies-in-the-foundations-of-statistics) _American Mathematical Monthly_ 85: 231--246.]

- Lin, Green, and Coppock (2016) write: "Covariates should generally be chosen on the basis of their expected ability to help predict outcomes, regardless of whether they appear well-balanced or imbalanced across treatment arms.
But there may be occasions when the covariate list specified in the PAP [pre-analysis plan] omitted a potentially important covariate (due to either an oversight or the need to keep the list short when N is small) with a nontrivial imbalance.
Protection against ex post bias (conditional on the observed imbalance) is then a legitimate concern." However, they recommend that if observed imbalances are allowed to influence the choice of covariates, "the balance checks and decisions about adjustment should be finalized before we see unblinded outcome data," "the _direction_ of the observed imbalance (e.g., whether the treatment group or the control group appears more advantaged at baseline) should not be allowed to influence decisions about adjustment," and the originally pre-specified estimator should "always be reported and labeled as such, even if alternative estimates are also reported."^[Winston Lin, Donald P. Green, and Alexander Coppock (2016), ["Standard Operating Procedures for Don Green’s Lab at Columbia,"](https://github.com/acoppock/Green-Lab-SOP) version 1.05, June 7. Italics in the original.]

7 When not to do it
==
It is a bad idea to adjust for covariates when you think those covariates could have been influenced by your treatment.
This is one of the reasons that many covariates are collected from baseline surveys; sometimes covariates that are collected from surveys after intervention could reflect the effects of the treatment rather than underlying characteristics of the subject.
Adjusting for covariates that are affected by the treatment---“post-treatment” covariates---can cause bias.

Suppose, for example, that Giné and Mansuri had collected data on how many political rallies a woman attended after receiving the treatment.
In estimating the treatment effect on independence of political choice, you may be tempted to include this variable as a covariate in your regression.
But including this variable, even if it strongly predicts the outcome, may distort the estimated effect of the treatment.

Let’s create this fake variable, which is correlated (like the outcome measure) with baseline covariates and also with treatment.
Here, by construction, the treatment effect on number of political rallies attended is 2.
When we included the rallies variable as a covariate, the estimated average treatment effect on independence of candidate choice averaged 0.54 across the 10,000 replications.
Recall that the true treatment effect on this outcome is 1.
This is severe bias, all because we controlled for a post-treatment covariate!^[The estimated bias is $-$ 0.459 with a margin of error (at the 95% confidence level) of 0.002.] This bias results from the fact that the covariate is correlated with treatment.

```{r, error=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
# Create post-treatment covariate that's correlated with pre-treatment covariates
rallies0 = round(.5*owns.id.card + has.formal.schooling + 1.5*TV.access + log(age))
rallies1 = rallies0 + 2
rallies.mat = rallies1 * Z.mat + rallies0 * (1-Z.mat)
 
# Estimate ATE with new model that includes the post-treatment covariate

adjust.for.post = function(Y, Z, X) {
  coef(lm(Y ~ Z + X + owns.id.card + has.formal.schooling + age + TV.access))[2]
}

post.adjusted.estimates = rep(NA, Replications)

for (i in 1:Replications) {
  post.adjusted.estimates[i]  =  adjust.for.post(Y.mat[,i], Z.mat[,i], rallies.mat[,i])
}

# Estimated bias of the new estimator
mean(post.adjusted.estimates) - true.treatment.effect

# Margin of error (at 95% confidence level) for the estimated bias
1.96 * sd(post.adjusted.estimates) / sqrt(Replications)
```

Just because you should not adjust for post-treatment covariates does not mean you cannot collect covariate data post-treatment, but you must exercise caution.
Some measures could be collected post-treatment but are unlikely to be affected by treatment (e.g., age and gender).
Be careful about measures that may be subject to evaluation-driven effects, though: for example, treated women may be more acutely aware of the expectation of political participation and may retrospectively report that they were more politically active than they actually were several years prior.

8 Concerns about small-sample bias
==
In small samples, regression adjustment may produce a biased estimate of the average treatment effect.^[David A. Freedman (2008), "On Regression Adjustments in Experiments with Several Treatments," _Annals of Applied Statistics_ 2: 176--196.
See also Winston Lin's blog posts ([part I](https://web.archive.org/web/20151024055802/http://blogs.worldbank.org/impactevaluations/node/847) and [part II](https://web.archive.org/web/20151024022122/http://blogs.worldbank.org/impactevaluations/node/849)) discussing his response to Freedman.] Some simulations have suggested that this bias tends to be negligible when the number of randomly assigned units is greater than twenty.[^7] If you’re working with a small sample, you may want to use an unbiased covariate adjustment method such as post-stratification (splitting the sample into subgroups based on the values of one or more baseline covariates, computing the treatment--control difference in mean outcomes for each subgroup, and taking a weighted average of these subgroup-specific treatment effect estimates, with weights proportional to sample size).^[Miratrix, Sekhon, and Yu (2013), cited above.]

[^7]: Green, Donald P. and Aronow, Peter M., Analyzing Experimental Data Using Regression: When Is Bias a Practical Concern? (March 7, 2011). Working paper: http://ssrn.com/abstract=1466886

9 How to make your covariate adjustment decisions transparent
==
In the interests of transparency, if you adjust for covariates, pre-specify your models and report both unadjusted and covariate-adjusted estimates.

The simulations above have demonstrated that results may change slightly or not-so-slightly depending on which covariates you choose to include in your model.
We’ve highlighted some rules of thumb here: include only pre-treatment covariates that are predictive of outcomes.
Deciding which covariates to include, though, is often a subjective rather than an objective enterprise, so another rule of thumb is to be totally transparent about your covariate decisions.
Always include the simplest model---the simple regression of outcome on treatment without controlling for covariates---in your paper or appendix to supplement the findings of your model including covariates.

Another way to minimize your readers’ concern that you went fishing for the particular combination of covariates that gave results favorable to your hypotheses is to pre-specify your models in a pre-analysis plan.^[For more discussion of pre-analysis plans, see, e.g., Benjamin A. Olken (2015), ["Promises and Perils of Pre-Analysis Plans,"](http://doi.org/10.1257/jep.29.3.61) _Journal of Economic Perspectives_ 29 (3): 61--80.] This gives you the opportunity to explain before you see the findings which pre-treatment covariates you expect to be predictive of the outcome.
You can even write these regressions out in R using fake data, as done here, so that when your results from the field arrive, all you need to do is run your code on the real data.
These efforts are a useful way of binding your own hands as a researcher and improving your credibility.

10 Covariates can help you investigate the integrity of the random assignment
==
Sometimes it is unclear whether random assignment actually occurred (or whether it occurred using the procedure that the researcher envisions).
 For example, when scholars analyze naturally occurring random assignments (e.g., those conducted by a government agency), it is useful to assess statistically whether the degree of imbalance between the treatment and control groups is within the expected margin of error.
 One statistical test is to regress treatment assignment on all of the covariates and calculate the F-statistic.
 The significance of this statistic can be assessed by simulating a large number of random assignments and for each one calculating the F-statistic; the resulting distribution can be used to calculate the p-value of the observed F-statistic.
 For example, if 10,000 simulations are conducted, and just 30 simulations generate an F-statistic larger than what one actually obtained from the data, the p-value is 0.003, which suggests that the observed level of imbalance is highly unusual.
In such cases, one may wish to investigate the randomization procedure more closely.

For further reading
==
Athey, Susan, and Guido W. Imbens (2017). "The Econometrics of Randomized Experiments." In _Handbook of Economic Field Experiments_, vol. 1 (E. Duflo and A. Banerjee, eds.). [arXiv](http://arxiv.org/abs/1607.00698) [DOI](http://dx.doi.org/10.1016/bs.hefe.2016.10.003)

Gerber, Alan S., and Donald P. Green (2012). _Field Experiments: Design, Analysis, and Interpretation_, chapter 4.

Hennessy, Jonathan, Tirthankar Dasgupta, Luke Miratrix, Cassandra Pattanayak, and Pradipta Sarkar (2016). "A Conditional Randomization Test to Account for Covariate Imbalance in Randomized Experiments." _Journal of Causal Inference_ 4: 61--80.

Judkins, David R., and Kristin E. Porter (2016). "Robustness of Ordinary Least Squares in Randomized Clinical Trials." _Statistics in Medicine_ 35: 1763--1773.

Lin, Winston (2012). "Regression Adjustment in Randomized Experiments: Is the Cure Really Worse than the Disease?" Development Impact blog post, [part I](https://web.archive.org/web/20151024055802/http://blogs.worldbank.org/impactevaluations/node/847) and [part II](https://web.archive.org/web/20151024022122/http://blogs.worldbank.org/impactevaluations/node/849).

Raudenbush, Stephen W. (1997). "Statistical Analysis and Optimal Design for Cluster Randomized Trials." _Psychological Methods_ 2: 173--185.

Wager, Stefan, Wenfei Du, Jonathan Taylor, and Robert Tibshirani (2016). "High-Dimensional Regression Adjustments in Randomized Experiments." _Proceedings of the National Academy of Sciences_ 113: 12673--12678. [arXiv](https://arxiv.org/abs/1607.06801) [DOI](http://doi.org/10.1073/pnas.1614732113)